{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e3098bd",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8b236f3-0e11-491e-baa0-6eebf0f3cfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ast\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import groupby\n",
    "\n",
    "from py_files.writer_director_to_one_hot import writer_director_to_one_hot\n",
    "from py_files.add_merge_begin_end_year import merge_start_end_year\n",
    "from py_files.load_box_office_data import load_and_aggregate_box_office\n",
    "from py_files.add_remake_feature import create_remake_column\n",
    "from py_files.add_langoriginaltitle_feature import add_language_of_original_title\n",
    "from py_files.add_ENvsNonEN_feature import add_english_title_or_not\n",
    "from py_files.add_movie_genre_feature import add_movie_genre\n",
    "from py_files.df_processor_enrichment import df_processor_enrichment\n",
    "\n",
    "from py_files.df_model_prep import df_model_prep\n",
    "from py_files.d2v_embed import d2v_embed\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "\n",
    "from pyspark.sql.functions import input_file_name, substring, udf,col, lit, coalesce,\\\n",
    "                                  when, regexp_replace, count, regexp_extract, split,\\\n",
    "                                  array_contains, monotonically_increasing_id, concat, concat_ws\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, ArrayType, FloatType\n",
    "\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7152c071",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c65a3cbf-7d58-4abc-b2ea-12feae0e9a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/21 21:48:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found files: train-1.csv, train-2.csv, train-3.csv, train-4.csv, train-5.csv, train-6.csv, train-7.csv, train-8.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.driver.bindAddress\",\"127.0.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "all_files = glob.glob(\"train-*.csv\")\n",
    "\n",
    "print(f\"Found files: {', '.join(all_files)}\")\n",
    "\n",
    "schema = StructType() \\\n",
    "      .add(\"_c0\",IntegerType(),True) \\\n",
    "      .add(\"tconst\",StringType(),True) \\\n",
    "      .add(\"primaryTitle\",StringType(),True) \\\n",
    "      .add(\"originalTitle\",StringType(),True) \\\n",
    "      .add(\"startYear\",IntegerType(),True) \\\n",
    "      .add(\"endYear\",IntegerType(),True) \\\n",
    "      .add(\"runtimeMinutes\",IntegerType(),True) \\\n",
    "      .add(\"numVotes\",IntegerType(),True) \\\n",
    "      .add(\"label\",BooleanType(),True)\n",
    "\n",
    "# skip the header and define our own because the automatic detection doesn't go right\n",
    "n_skip_rows = 1\n",
    "row_rdd = spark.sparkContext \\\n",
    "    .textFile(\"train-*.csv\") \\\n",
    "    .zipWithIndex() \\\n",
    "    .filter(lambda row: row[1] >= n_skip_rows) \\\n",
    "    .map(lambda row: row[0])\n",
    "\n",
    "training_data = spark.read.csv(row_rdd, schema=schema, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba87546d",
   "metadata": {},
   "source": [
    "# Preprocessing of original columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b98bd2a2-59b1-4554-b5ef-45fbeee1ae11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------------------+--------------------+---------+-------+--------------+--------+-----+\n",
      "|_c0|   tconst|        primaryTitle|       originalTitle|startYear|endYear|runtimeMinutes|numVotes|label|\n",
      "+---+---------+--------------------+--------------------+---------+-------+--------------+--------+-----+\n",
      "|  4|tt0010600|            The Doll|           Die Puppe|     1919|   null|            66|    null| true|\n",
      "|  7|tt0011841|       Way Down East|       Way Down East|     1920|   null|           145|    null| true|\n",
      "|  9|tt0012494|             Déstiny|        Der müde Tod|     1921|   null|            97|    null| true|\n",
      "| 25|tt0015163|       The Navigator|       The Navigator|     1924|   null|            59|    null| true|\n",
      "| 38|tt0016220|The Phantom of th...|The Phantom of th...|     1925|   null|            93|    null| true|\n",
      "| 42|tt0016630|     Báttling Bútlér|     Battling Butler|     1926|   null|            77|    null| true|\n",
      "| 81|tt0021015|Juno and the Paycock|                null|     1929|   null|            85|    null|false|\n",
      "|118|tt0023973|Thé Éáglé ánd thé...|                null|     1933|   null|            73|    null| true|\n",
      "|119|tt0023986| Émplớyéés' Éntráncé|                null|     1933|   null|            75|    null| true|\n",
      "|123|tt0024184|   The Invisible Man|   The Invisible Man|     1933|   null|            71|    null| true|\n",
      "|125|tt0024216|           King Kong|           King Kong|     1933|   null|           100|    null| true|\n",
      "|135|tt0025028|               Dames|                null|     1934|   null|            91|    null| true|\n",
      "|163|tt0027478|The Crime of Mons...|Le crime de Monsi...|     1936|   null|            80|    null| true|\n",
      "|180|tt0028333|          Swing Timé|                null|     1936|   null|           103|    null| true|\n",
      "|215|tt0030341|   The Lady Vanishes|   The Lady Vanishes|     1938|   null|            96|    null| true|\n",
      "|222|tt0031143|The Cat and the C...|                null|     1939|   null|            72|    null| true|\n",
      "|231|tt0031385|  Goodbye, Mr. Chips|                null|     1939|   null|           114|    null| true|\n",
      "|239|tt0031647|            Midnight|                null|     1939|   null|            94|    null| true|\n",
      "|242|tt0031762|Only Angels Have ...|                null|     1939|   null|           121|    null| true|\n",
      "|253|tt0032156|The Story of the ...|  Zangiku monogatari|     1939|   null|           143|    null| true|\n",
      "+---+---------+--------------------+--------------------+---------+-------+--------------+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: int, tconst: string, primaryTitle: string, originalTitle: string, startYear: int, endYear: int, runtimeMinutes: int, numVotes: int, label: boolean, primaryTitleFormatted: string, Year: int]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_titles(title):\n",
    "    return unicodedata.normalize('NFKD',title.lower()).encode('ascii', errors='ignore').decode('utf-8').replace(\" \", \"_\").replace(\"\\W\", \"\")\n",
    "\n",
    "udf_format_titles = udf(format_titles, StringType()) # if the function returns an int\n",
    "\n",
    "training_data.show()\n",
    "training_data = training_data.withColumn(\"primaryTitleFormatted\", lit(udf_format_titles('primaryTitle')))\n",
    "training_data = training_data.withColumn('Year', coalesce('startYear', 'endYear'))\n",
    "training_data.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3627c6f5",
   "metadata": {},
   "source": [
    "## Preprocessing of exogenous data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494afa9b",
   "metadata": {},
   "source": [
    "### Oscar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7ec23c0-92ac-4805-aeff-04f254f79d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "oscars = spark.read.csv(\"additional_data/oscars.csv\", header=True)\n",
    "oscars = oscars.na.drop(subset=[\"film\"])\n",
    "oscars = oscars.withColumn(\"film\", lit(udf_format_titles('film')))\n",
    "\n",
    "cond = [training_data.primaryTitleFormatted == oscars.film]\n",
    "oscar_noms = training_data.join(oscars, cond, 'inner').groupBy('tconst').count()\n",
    "oscar_wins = training_data.join(oscars, cond, 'inner').filter(col('winner') == True).groupBy('tconst').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7a4f6ab-f29a-458e-b470-bdf4dae90021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oscar_noms.show()\n",
    "# oscar_wins.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20072838",
   "metadata": {},
   "source": [
    "### Razzie data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64bd5b76-d78d-4661-97b9-877de5004661",
   "metadata": {},
   "outputs": [],
   "source": [
    "razzies = spark.read.csv(\"additional_data/Razzies.csv\", header=True)\n",
    "razzies = razzies.na.drop(subset=[\"moviename\"])\n",
    "razzies = razzies.withColumn(\"moviename\", lit(udf_format_titles('moviename')))\n",
    "\n",
    "cond = [training_data.primaryTitleFormatted == razzies.moviename]\n",
    "razzie_noms = training_data.join(razzies, cond, 'inner').groupBy('tconst').count()\n",
    "razzie_wins = training_data.join(razzies, cond, 'inner').filter(col('Wins') == True).groupBy('tconst').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "244055bf-6a20-4b23-b89d-af980e1be66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# razzie_noms.show()\n",
    "# razzie_wins.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19829c71",
   "metadata": {},
   "source": [
    "### Writer and Director data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f396243-fcc8-43e5-8e63-0c77e428cb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writers = writer_director_to_one_hot(\"writers\")\n",
    "# directors = writer_director_to_one_hot(\"directors\")\n",
    "# written_and_directed = writers.add(directors, fill_value=0).fillna(0).astype(int).loc[df_preprocessed[\"tconst\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d2a0f4",
   "metadata": {},
   "source": [
    "### TMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "59db17df-c52f-4f57-b3f0-d37992ff55ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "schema = StructType() \\\n",
    "      .add(\"id\",IntegerType(),True) \\\n",
    "      .add(\"belongs_to_collection\",StringType(),True) \\\n",
    "      .add(\"budget\",IntegerType(),True) \\\n",
    "      .add(\"genres\",StringType(),True) \\\n",
    "      .add(\"homepage\",StringType(),True) \\\n",
    "      .add(\"imdb_id\",StringType(),True) \\\n",
    "      .add(\"original_language\",StringType(),True) \\\n",
    "      .add(\"original_title\",StringType(),True) \\\n",
    "      .add(\"overview\",StringType(),True) \\\n",
    "      .add(\"popularity\",FloatType(),True) \\\n",
    "      .add(\"poster_page\",StringType(),True) \\\n",
    "      .add(\"production_companies\",StringType(),True) \\\n",
    "      .add(\"production_countries\",StringType(),True) \\\n",
    "      .add(\"release_data\",StringType(),True) \\\n",
    "      .add(\"runtime\",IntegerType(),True) \\\n",
    "      .add(\"spoken_language\",StringType(),True) \\\n",
    "      .add(\"status\",StringType(),True) \\\n",
    "      .add(\"tagline\",StringType(),True) \\\n",
    "      .add(\"title\",StringType(),True) \\\n",
    "      .add(\"Keywords\",StringType(),True) \\\n",
    "      .add(\"cast\",StringType(),True) \\\n",
    "      .add(\"crew\",StringType(),True) \\\n",
    "      .add(\"revenue\",IntegerType(),True)\n",
    "\n",
    "n_skip_rows = 1\n",
    "row_rdd = spark.sparkContext \\\n",
    "    .textFile(\"additional_data/TMDB.csv\") \\\n",
    "    .zipWithIndex() \\\n",
    "    .filter(lambda row: row[1] >= n_skip_rows) \\\n",
    "    .map(lambda row: row[0])\n",
    "\n",
    "df_TMDB = spark.read.csv(row_rdd, header=False, escape=\"\\\"\", schema=schema).select(\"budget\", \"genres\", \"imdb_id\", \n",
    "                                                                             \"original_language\", \"overview\", \n",
    "                                                                             \"popularity\", \"production_companies\", \n",
    "                                                                             \"tagline\", \"Keywords\", \"revenue\")\n",
    "\n",
    "# # I think there are some incorrect rows present due to loading errors. \n",
    "# df_TMDB = spark.read.csv(\"additional_data/TMDB.csv\", header=True, escape=\"\\\"\")[[\"budget\", \"genres\", \"imdb_id\", \n",
    "#                                                                                 \"original_language\", \"overview\", \n",
    "#                                                                                 \"popularity\", \"production_companies\", \n",
    "#                                                                                 \"tagline\", \"Keywords\", \"revenue\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d8a6790c-3ba8-437f-8171-a95020176d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_list(dictionary):\n",
    "    try:\n",
    "        d = ast.literal_eval(dictionary)\n",
    "    except ValueError:\n",
    "        return []\n",
    "        \n",
    "    try:\n",
    "        return [i[\"name\"] for i in d]\n",
    "    except TypeError:\n",
    "        return []\n",
    "\n",
    "udf_dict_to_list = udf(lambda x: dict_to_list(x), ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5299fd8a-7476-4d31-a514-40e19b8c12a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TMDB = df_TMDB.withColumn(\"genres\", udf_dict_to_list(col(\"genres\")))\n",
    "df_TMDB = df_TMDB.withColumn(\"Keywords\", udf_dict_to_list(col(\"Keywords\")))\n",
    "df_TMDB = df_TMDB.withColumn(\"production_companies\", udf_dict_to_list(col(\"production_companies\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "35d2e702-ef36-47ef-94da-113333c9b806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------+-----------------+--------------------+----------+--------------------+--------------------+--------------------+--------+\n",
      "|  budget|              genres|  imdb_id|original_language|            overview|popularity|production_companies|             tagline|            Keywords| revenue|\n",
      "+--------+--------------------+---------+-----------------+--------------------+----------+--------------------+--------------------+--------------------+--------+\n",
      "|14000000|            [Comedy]|tt2637294|               en|When Lou, who has...|  6.575393|[Paramount Pictur...|The Laws of Space...|[time travel, seq...|12314651|\n",
      "|40000000|[Comedy, Drama, F...|tt0368933|               en|Mia Thermopolis i...|  8.248895|[Walt Disney Pict...|It can take a lif...|[coronation, duty...|95149435|\n",
      "| 3300000|             [Drama]|tt2582802|               en|Under the directi...|  64.29999|[Bold Films, Blum...|The road to great...|[jazz, obsession,...|13092000|\n",
      "| 1200000|   [Thriller, Drama]|tt1821480|               hi|Vidya Bagchi (Vid...|  3.174936|                  []|                null|[mystery, bollywo...|16000000|\n",
      "|       0|  [Action, Thriller]|tt1380152|               ko|Marine Boy is the...|   1.14807|                  []|                null|                  []| 3923970|\n",
      "+--------+--------------------+---------+-----------------+--------------------+----------+--------------------+--------------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_TMDB.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b736e978",
   "metadata": {},
   "source": [
    "### Metacritic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4666beb4-d80f-4691-a83c-aff094086af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema2 = StructType() \\\n",
    "      .add(\"_c0\",IntegerType(),True) \\\n",
    "      .add(\"tconst\",StringType(),True) \\\n",
    "      .add(\"genres\",StringType(),True) \\\n",
    "      .add(\"language\",StringType(),True) \\\n",
    "      .add(\"overview\",StringType(),True) \\\n",
    "\n",
    "n_skip_rows = 1\n",
    "row_rdd2 = spark.sparkContext \\\n",
    "    .textFile(\"additional_data/Metacritic.csv\") \\\n",
    "    .zipWithIndex() \\\n",
    "    .filter(lambda row: row[1] >= n_skip_rows) \\\n",
    "    .map(lambda row: row[0])\n",
    "\n",
    "df_meta = spark.read.csv(row_rdd2, header=False, escape=\"\\\"\", schema=schema2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4cba816e-ec61-40b4-9fd1-8dec98dc64c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_overview(overview):\n",
    "    overview = eval(overview)\n",
    "        \n",
    "    if overview:\n",
    "        return overview[0]\n",
    "    else:\n",
    "        return \" \"\n",
    "    \n",
    "udf_generate_overview = udf(lambda x: generate_overview(x), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6217d1de-3ed5-4145-931c-a205542291b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = df_meta.withColumn(\"overview_meta\", udf_generate_overview(col(\"overview\"))).drop(\"overview\")\n",
    "df_meta = df_meta.withColumnRenamed(\"genres\", \"genres_meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7ac58214-52ac-4aa8-a6e1-401e70c357f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------------------+--------------------+--------------------+\n",
      "|_c0|   tconst|         genres_meta|            language|       overview_meta|\n",
      "+---+---------+--------------------+--------------------+--------------------+\n",
      "|  0|tt0024184|['Sci-Fi', 'Horror']|         ['English']|A scientist finds...|\n",
      "|  1|tt0024216|['Adventure', 'Sc...|         ['English']|A film crew trave...|\n",
      "|  2|tt0028333|['Comedy', 'Roman...|['English', 'Fren...|A performer and g...|\n",
      "|  3|tt0030341|['Mystery', 'Thri...|['English', 'Germ...|While travelling ...|\n",
      "|  4|tt0031762|['Adventure', 'Dr...|['English', 'Span...|At a remote South...|\n",
      "+---+---------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_meta.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b466a52c-b03e-4a5e-b460-fac18195d525",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = [df_meta.tconst == df_TMDB.imdb_id]\n",
    "full = df_TMDB.join(df_meta, cond, 'outer')\n",
    "\n",
    "overviews = full.select(concat_ws(\"\", full.overview, full.overview_meta).alias(\"FullOverview\"), \"imdb_id\", \"tconst\")\n",
    "\n",
    "# overviews = full.select(concat_ws(\"\", full.overview, full.overview_meta).alias(\"FullOverview\"), \"imdb_id\", \"tconst\", \n",
    "#                                                                                \"budget\", \"genres\", \"genres_meta\", \"original_language\", \n",
    "#                                                                                \"language\", \"popularity\", \"Keywords\", \"revenue\")\n",
    "\n",
    "# Some nonsense data is still present, but I think it won't be an issue after joining on training_data\n",
    "overviews = overviews.withColumn('movie_id', coalesce('tconst', 'imdb_id')).drop(\"tconst\", \"imdb_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cd7c22",
   "metadata": {},
   "source": [
    "### Box Office data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0267589",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_office_schema = StructType() \\\n",
    "      .add(\"Rank\",IntegerType(),True) \\\n",
    "      .add(\"Release Group\",StringType(),True) \\\n",
    "      .add(\"Worldwide\",StringType(),True) \\\n",
    "      .add(\"Domestic\",StringType(),True) \\\n",
    "      .add(\"Col_to_Drop1\",StringType(),True) \\\n",
    "      .add(\"Foreign\",StringType(),True) \\\n",
    "      .add(\"Col_to_Drop2\",StringType(),True)\n",
    "\n",
    "n_skip_rows = 1\n",
    "box_office_rdd = spark.sparkContext \\\n",
    "    .textFile(\"box_office_mojo/*.csv\") \\\n",
    "    .zipWithIndex() \\\n",
    "    .filter(lambda row: row[1] >= n_skip_rows) \\\n",
    "    .map(lambda row: row[0])\n",
    "\n",
    "df_box_office_mojo = spark.read.csv(box_office_rdd, schema=box_office_schema, header=False)\n",
    "\n",
    "# process the 'release group' (read movie title) in the same way as the formatted title\n",
    "df_box_office_mojo = df_box_office_mojo.withColumn(\"Release Group\", lit(udf_format_titles('Release Group')))\n",
    "\n",
    "# add the year of the box office file\n",
    "df_box_office_mojo = df_box_office_mojo.withColumn(\"year\", substring(input_file_name(), -8, 4).cast(IntegerType()))\n",
    "\n",
    "# drop unnecessary columns\n",
    "df_box_office_mojo = df_box_office_mojo.drop(*('Col_to_Drop1', 'Col_to_Drop2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501d2746",
   "metadata": {},
   "source": [
    "# Adding of exogenous columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7402df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_incl_exog = df_preprocessed.copy(deep=True)\n",
    "# df_incl_exog = df_incl_exog.rename({\"tconst\" : \"id\"}, axis = 1).set_index(\"id\")\n",
    "# df_incl_exog.info()\n",
    "# training_data = training_data.withColumnRenamed('tconst', 'id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bd06e1",
   "metadata": {},
   "source": [
    "## add oscar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeafbe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.join(oscar_noms, ['tconst'], 'left').withColumnRenamed('count', 'oscar_noms')\n",
    "training_data = training_data.join(oscar_wins, ['tconst'], 'left').withColumnRenamed('count', 'oscar_wins')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a84eb1c",
   "metadata": {},
   "source": [
    "## add razzie data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0cc288",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.join(razzie_noms, ['tconst'], 'left').withColumnRenamed('count', 'razzie_noms')\n",
    "training_data = training_data.join(razzie_wins, ['tconst'], 'left').withColumnRenamed('count', 'razzie_wins')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c17320-e41c-40a1-a9b1-2f9ddb04c8ee",
   "metadata": {},
   "source": [
    "## add overviews\n",
    "\n",
    "#### Not sure if working properly, since I keep running out of RAM to test lmao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e647c9c-ffd8-4168-8f0d-475f3871bc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = [training_data.tconst == overviews.movie_id]\n",
    "\n",
    "training_data = training_data.join(overviews, cond, \"left\").drop(\"movie_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac09b811",
   "metadata": {},
   "source": [
    "## add mojo box office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66854cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missing_box_office_values(column):\n",
    "    return when(column != '-', column).otherwise(lit(None))\n",
    "\n",
    "cond_mojo_merge = [training_data.primaryTitleFormatted == df_box_office_mojo['Release Group'], training_data.Year == df_box_office_mojo.year]\n",
    "\n",
    "training_data = training_data.join(df_box_office_mojo, cond_mojo_merge, 'left').drop(*('Release Group', 'year'))\n",
    "training_data = training_data.withColumn(\"Worldwide\", remove_missing_box_office_values(col(\"Worldwide\")))\n",
    "training_data = training_data.withColumn(\"Domestic\", remove_missing_box_office_values(col(\"Domestic\")))\n",
    "training_data = training_data.withColumn(\"Foreign\", remove_missing_box_office_values(col(\"Foreign\")))\n",
    "training_data = training_data.withColumn('Worldwide', regexp_replace('Worldwide', '[$,]', '').cast('double'))\n",
    "training_data = training_data.withColumn('Domestic', regexp_replace('Domestic', '[$,]', '').cast('double'))\n",
    "training_data = training_data.withColumn('Foreign', regexp_replace('Foreign', '[$,]', '').cast('double'))\n",
    "training_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264c2789",
   "metadata": {},
   "source": [
    "## add remake column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b895e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.join(\n",
    "    training_data.groupBy(\"primaryTitle\").agg((count(\"*\")>1).cast(\"int\").alias(\"hasRemake\")),\n",
    "    on=\"primaryTitle\",\n",
    "    how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec691609",
   "metadata": {},
   "source": [
    "## add title language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a0e548-2a06-4d02-8714-344aa9220042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add the language of the original title, currently commented for training data usage and not wait 15 min every time\n",
    "# df_incl_exog = add_language_of_original_title(df_incl_exog)\n",
    "\n",
    "# df_added_lang = pd.read_csv('additional_data/df_added_lang.csv', index_col=0)\n",
    "# df_added_lang = df_added_lang.rename({\"tconst\" : \"id\"}, axis = 1).set_index(\"id\")\n",
    "# df_incl_exog = df_incl_exog.join(df_added_lang['title_language'], how='left')\n",
    "\n",
    "added_lang_schema = StructType() \\\n",
    "      .add(\"_c0\",IntegerType(),True) \\\n",
    "      .add(\"tconst\",StringType(),True) \\\n",
    "      .add(\"primaryTitle\",StringType(),True) \\\n",
    "      .add(\"originalTitle\",StringType(),True) \\\n",
    "      .add(\"startYear\",IntegerType(),True) \\\n",
    "      .add(\"endYear\",IntegerType(),True) \\\n",
    "      .add(\"runtimeMinutes\",IntegerType(),True) \\\n",
    "      .add(\"numVotes\",IntegerType(),True) \\\n",
    "      .add(\"label\",BooleanType(),True) \\\n",
    "      .add(\"title_language\",StringType(),True) \\\n",
    "      .add(\"isEN\",BooleanType(),True) \n",
    "\n",
    "n_skip_rows = 1\n",
    "added_lang_rdd = spark.sparkContext \\\n",
    "    .textFile('additional_data/df_added_lang.csv') \\\n",
    "    .zipWithIndex() \\\n",
    "    .filter(lambda row: row[1] >= n_skip_rows) \\\n",
    "    .map(lambda row: row[0])\n",
    "\n",
    "df_added_lang = spark.read.csv(added_lang_rdd, schema=added_lang_schema, header=False)\n",
    "\n",
    "training_data = training_data.join(df_added_lang.select(['tconst', 'title_language']), on='tconst', how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa17831",
   "metadata": {},
   "source": [
    "## add whether title is English or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273f5f4a-8ba2-4346-bef1-4ee17167dc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indicate_whether_language_is_english(column):\n",
    "    return when(column == 'en', True).otherwise(lit(False))\n",
    "\n",
    "training_data = training_data.withColumn(\"isEN\", indicate_whether_language_is_english(col(\"title_language\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b3d3d8-0479-4491-833f-bc5a9783cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a68ab6",
   "metadata": {},
   "source": [
    "## add movie genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deae718-3950-472c-9973-29fbe3dedf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_incl_exog = add_movie_genre(df_incl_exog)\n",
    "\n",
    "def retrieve_year(string):\n",
    "    try:\n",
    "        return int(re.search('\\((.*?)\\)', string).group()[1:-1])\n",
    "    except:\n",
    "        return pd.NA\n",
    "\n",
    "def remove_year(string):\n",
    "    try:\n",
    "        return re.sub('\\((.*?)\\)', '', string)[:-1]\n",
    "    except:\n",
    "        return str\n",
    "\n",
    "def add_movie_genre(df_):\n",
    "    ''''Create onehot encoded features of genres'''\n",
    "    \n",
    "    # load movies with genre data\n",
    "    movie_genres = pd.read_csv(r'additional_data/movie_genres.csv', index_col=0)\n",
    "\n",
    "    # remove movies in data set that don't have genres\n",
    "    movie_genres = movie_genres[movie_genres['genres'] != '(no genres listed)']\n",
    "    \n",
    "    # get date for each movie from title column\n",
    "    movie_genres['year'] = movie_genres['title'].apply(lambda x: retrieve_year(x))\n",
    "    movie_genres = movie_genres.dropna(subset=['year'])\n",
    "\n",
    "    # remove year from title column and set title data type correctly\n",
    "    movie_genres['year'] = movie_genres['year'].astype(int)\n",
    "    movie_genres['title'] = movie_genres['title'].apply(lambda x: remove_year(x)).astype('string')\n",
    "    movie_genres['genres'] = movie_genres['genres'].apply(lambda x: x.split('|'))\n",
    "    \n",
    "    # format title in same way as original dataset\n",
    "    movie_genres[\"titleFormatted\"] = movie_genres[\"title\"].str.lower()\\\n",
    "                                       .str.normalize('NFKD')\\\n",
    "                                       .str.encode('ascii', errors='ignore')\\\n",
    "                                       .str.decode('utf-8')\\\n",
    "                                       .str.replace(\" \", \"_\", regex=True)\\\n",
    "                                       .str.replace(\"\\W\", \"\", regex=True)\n",
    "    \n",
    "    movie_genres.drop_duplicates(subset=['titleFormatted', 'year'], inplace=True)\n",
    "    \n",
    "    df_ = df_.reset_index().merge(movie_genres[['year', 'titleFormatted', 'genres']], left_on=['primaryTitleFormatted', 'Year'], right_on=['titleFormatted', 'year'], how='left').set_index('id')\n",
    "    s = df_['genres'].explode()\n",
    "    df_ = df_.join(pd.crosstab(s.index, s), how='left')\n",
    "    \n",
    "    return df_\n",
    "    \n",
    "movie_genres = spark.read.csv(\"additional_data/movie_genres.csv\", header=True)\n",
    "movie_genres = movie_genres.filter(movie_genres.genres != '(no genres listed)')\n",
    "movie_genres = movie_genres.filter(movie_genres.title.endswith(')'))\n",
    "movie_genres = movie_genres.withColumn('year', substring(col('title'), -5, 4))\n",
    "movie_genres = movie_genres.filter(movie_genres.year != '')\n",
    "movie_genres = movie_genres.withColumn('year', col('year').cast(IntegerType()))\n",
    "movie_genres = movie_genres.withColumn('title', regexp_replace(col('title'), r' \\(.*?\\)', ''))\n",
    "movie_genres = movie_genres.withColumn('genres', split(col('genres'), '\\|'))\n",
    "movie_genres.show()\n",
    "\n",
    "movie_genres = movie_genres.withColumn(\"titleFormatted\", lit(udf_format_titles('title')))\n",
    "movie_genres = movie_genres.dropDuplicates(['titleFormatted', 'year'])\n",
    "\n",
    "list_of_genres = ['Action', 'Adventure', 'Animation', 'Children', 'Comedy',\n",
    "       'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',\n",
    "       'IMAX', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War',\n",
    "       'Western']\n",
    "\n",
    "for c in list_of_genres:\n",
    "    movie_genres = movie_genres.withColumn(c, array_contains(\"genres\", c).cast(\"int\"))\n",
    "\n",
    "movie_genres.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f5c2b1",
   "metadata": {},
   "source": [
    "## add writers and directors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc7cde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incl_exog = pd.concat([df_incl_exog.T, written_and_directed.T]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3c4725",
   "metadata": {},
   "source": [
    "## add TMDB & Metacritic overviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f90bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incl_exog = pd.merge(df_incl_exog, df_TMDB, how = \"left\", left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ca98fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incl_exog[\"overview\"].str.len().sort_values().dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206a74ec",
   "metadata": {},
   "source": [
    "## save dataframe with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e090e7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incl_exog.to_csv('df_with_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eedb0b6",
   "metadata": {},
   "source": [
    "# Preparing data for classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1874804",
   "metadata": {},
   "source": [
    "Convert non-numeric columns to numeric.\n",
    "We use Doc2Vec to embed each string column into n-by-128 array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c1330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('df_with_features.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c9da0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_prepped = df_model_prep(train_df,'train')\n",
    "train_df_prepped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d8449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_model_prep function for demonstration purposes\n",
    "#\n",
    "# from py_files.d2v_embed import d2v_embed\n",
    "# import pandas as pd\n",
    "# import math\n",
    "\n",
    "# def df_model_prep(df, filename):\n",
    "    \n",
    "#     try:\n",
    "#         print(\"Looking for pre made file...\")\n",
    "#         return pd.read_csv(f\"{filename}_df_with_features_fully_processed_read_for_model.csv\", index_col = 0)\n",
    "#     except:\n",
    "#         print(\"No file found, creating a new one\")\n",
    "    \n",
    "#     prim_title_df = d2v_embed(df['primaryTitle'])\n",
    "#     orig_title_df = d2v_embed(df['originalTitle'])\n",
    "#     prim_title_formatted_df = d2v_embed(df['primaryTitleFormatted'])\n",
    "#     title_formatted_df = d2v_embed(df['titleFormatted'])\n",
    "#     genres_df = d2v_embed(df['genres'])\n",
    "\n",
    "#     # just encode languages into ints for this column\n",
    "#     df['title_language'] = pd.factorize(df['title_language'])[0]\n",
    "\n",
    "#     df.drop(columns = df.select_dtypes(include='object').columns, inplace=True)\n",
    "\n",
    "#     # dealing with (some) nan values\n",
    "#     for index, row in df.iterrows():\n",
    "#         # For missing startYear or endYear entries, insert the other, if it exists.\n",
    "#         if math.isnan(row['startYear']):\n",
    "#             if not math.isnan(row['endYear']):\n",
    "#                 df.at[index,'startYear']=df.at[index,'endYear']\n",
    "#         if math.isnan(row['endYear']):\n",
    "#             if not math.isnan(row['startYear']):\n",
    "#                 df.at[index,'endYear']=df.at[index,'startYear']\n",
    "\n",
    "#         # For missing oscar_noms and oscar_wins, insert 0\n",
    "#         if math.isnan(row['oscar_noms']):\n",
    "#             df.at[index,'oscar_noms'] = 0\n",
    "#         if math.isnan(row['oscar_wins']):\n",
    "#             df.at[index,'oscar_wins'] = 0\n",
    "#         if math.isnan(row['razzie_noms']):\n",
    "#             df.at[index,'razzie_noms'] = 0\n",
    "#         if math.isnan(row['razzie_wins']):\n",
    "#             df.at[index,'razzie_wins'] = 0\n",
    "\n",
    "#     df['numVotes'] = df['numVotes'].fillna(df['numVotes'].mean(skipna=True))\n",
    "#     df['runtimeMinutes'] = df['runtimeMinutes'].fillna(df['runtimeMinutes'].mean(skipna=True))\n",
    "    \n",
    "#     df['title_language'] = pd.factorize(df['title_language'])[0]\n",
    "    \n",
    "#     df = df.join(prim_title_df)\n",
    "#     df = df.join(orig_title_df)\n",
    "#     df = df.join(prim_title_formatted_df)\n",
    "#     df = df.join(title_formatted_df)\n",
    "#     df = df.join(genres_df)\n",
    "    \n",
    "#     df.to_csv(f\"{filename}_df_with_features_fully_processed_read_for_model.csv\")\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc54e3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d2v_embed function for demonstration purposes\n",
    "# \n",
    "# from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# import multiprocessing as mp\n",
    "# from tqdm import tqdm\n",
    "# import pandas as pd\n",
    "# import math\n",
    "\n",
    "# def d2v_embed(df_col, max_epochs = 100, vec_size = 128, alpha = 0.025):\n",
    "    \n",
    "#     df_col = df_col.fillna(\" \")\n",
    "#     df_col = df_col.str.lower()\\\n",
    "#                    .str.normalize('NFKD')\\\n",
    "#                    .str.encode('ascii', errors='ignore')\\\n",
    "#                    .str.decode('utf-8')\\\n",
    "#                    .str.replace(\"\\W\", \" \", regex=True)\n",
    "    \n",
    "#     tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(df_col)]\n",
    "\n",
    "#     model = Doc2Vec(vector_size=vec_size,\n",
    "#                     alpha=alpha, \n",
    "#                     min_alpha=0.00025,\n",
    "#                     min_count=1,\n",
    "#                     dm =1,\n",
    "#                     workers = mp.cpu_count())\n",
    "  \n",
    "#     model.build_vocab(tagged_data)\n",
    "\n",
    "#     for epoch in tqdm(range(max_epochs)):\n",
    "#     #     print('iteration {0}'.format(epoch))\n",
    "#         model.train(tagged_data,\n",
    "#                     total_examples=model.corpus_count,\n",
    "#                     epochs=model.epochs)\n",
    "#         # decrease the learning rate\n",
    "#         model.alpha -= 0.0002\n",
    "#         # fix the learning rate, no decay\n",
    "#         model.min_alpha = model.alpha\n",
    "    \n",
    "#     # save model\n",
    "#     model.save(f\"doc2vec_model_{df_col.name}.model\")\n",
    "    \n",
    "#     #return df with doc embeddings\n",
    "#     return pd.DataFrame([model.docvecs[i] for i in range(len(df_col))], \n",
    "#                         index = df_col.index,\n",
    "#                         columns = [f\"{df_col.name}_{i}\" for i in range(vec_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a5e571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_processor_enrichment function for demonstration purposes\n",
    "# \n",
    "# import json\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from itertools import groupby\n",
    "\n",
    "# from py_files.writer_director_to_one_hot import writer_director_to_one_hot\n",
    "# from py_files.add_merge_begin_end_year import merge_start_end_year\n",
    "# from py_files.load_box_office_data import load_and_aggregate_box_office\n",
    "# from py_files.add_remake_feature import create_remake_column\n",
    "# from py_files.add_langoriginaltitle_feature import add_language_of_original_title\n",
    "# from py_files.add_ENvsNonEN_feature import add_english_title_or_not\n",
    "# from py_files.add_movie_genre_feature import add_movie_genre\n",
    "\n",
    "# from py_files.d2v_embed import d2v_embed\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import lightgbm as lgb\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# import math\n",
    "\n",
    "# def df_processor_enrichment(filename):\n",
    "    \n",
    "#     try:\n",
    "#         print(\"Looking for pre made file...\")\n",
    "#         return pd.read_csv(f\"{filename}_df_with_features.csv\", index_col = 0)\n",
    "#     except:\n",
    "#         print(\"File not found, creating a new one..\")\n",
    "              \n",
    "#     df_original = pd.read_csv(filename, index_col=0)\n",
    "#     # df_original.head()\n",
    "\n",
    "#     # start the preprocessing\n",
    "#     df_preprocessed = df_original.replace(\"\\\\N\", np.nan)\n",
    "#     df_preprocessed[\"primaryTitleFormatted\"] = df_preprocessed[\"primaryTitle\"].str.lower()\\\n",
    "#                                                                               .str.normalize('NFKD')\\\n",
    "#                                                                               .str.encode('ascii', errors='ignore')\\\n",
    "#                                                                               .str.decode('utf-8')\\\n",
    "#                                                                               .str.replace(\" \", \"_\", regex=True)\\\n",
    "#                                                                               .str.replace(\"\\W\", \"\", regex=True)\n",
    "\n",
    "#     # merge endYear into beginYear when beginYear is not available --> rename Year\n",
    "#     df_preprocessed = merge_start_end_year(df_preprocessed)\n",
    "\n",
    "#     # set the datatypes of the dataframe correctly\n",
    "#     df_preprocessed['Year'] = df_preprocessed['Year'].astype(int)\n",
    "#     df_preprocessed['runtimeMinutes'] = df_preprocessed['runtimeMinutes'].astype(float)\n",
    "\n",
    "#     # df_preprocessed.info()\n",
    "\n",
    "\n",
    "#     oscars = pd.read_csv(\"additional_data/oscars.csv\")\n",
    "\n",
    "#     oscars[\"film\"] = oscars[\"film\"].str.lower()\\\n",
    "#                                    .str.normalize('NFKD')\\\n",
    "#                                    .str.encode('ascii', errors='ignore')\\\n",
    "#                                    .str.decode('utf-8')\\\n",
    "#                                    .str.replace(\" \", \"_\", regex=True)\\\n",
    "#                                    .str.replace(\"\\W\", \"\", regex=True)\n",
    "\n",
    "#     # Counting oscar nominations and wins per movie\n",
    "#     oscar_noms = pd.merge(df_preprocessed, oscars, left_on = \"primaryTitleFormatted\", right_on = \"film\").groupby(\"tconst\")[\"winner\"].count()\n",
    "#     oscar_wins = pd.merge(df_preprocessed, oscars, left_on = \"primaryTitleFormatted\", right_on = \"film\").groupby(\"tconst\")[\"winner\"].sum()\n",
    "\n",
    "\n",
    "#     # Find writers and directors per movie and combine the two\n",
    "#     written_and_directed = (writer_director_to_one_hot(\"writers\") + writer_director_to_one_hot(\"directors\")).fillna(0).astype(int).loc[df_preprocessed['tconst']]\n",
    "\n",
    "\n",
    "#     df_box_office_mojo = load_and_aggregate_box_office()\n",
    "\n",
    "#     # process the 'release group' (read movie title) in the same way as the formatted title\n",
    "#     df_box_office_mojo[\"Release Group\"] = df_box_office_mojo[\"Release Group\"].str.lower()\\\n",
    "#                                            .str.normalize('NFKD')\\\n",
    "#                                            .str.encode('ascii', errors='ignore')\\\n",
    "#                                            .str.decode('utf-8')\\\n",
    "#                                            .str.replace(\" \", \"_\", regex=True)\\\n",
    "#                                            .str.replace(\"\\W\", \"\", regex=True)\n",
    "#     df_box_office_mojo.drop(['%', '%.1'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "#     df_incl_exog = df_preprocessed.copy(deep=True)\n",
    "#     df_incl_exog = df_incl_exog.rename({\"tconst\" : \"id\"}, axis = 1).set_index(\"id\")\n",
    "#     # df_incl_exog.info()\n",
    "\n",
    "\n",
    "#     df_incl_exog[\"oscar_noms\"] = oscar_noms\n",
    "#     df_incl_exog[\"oscar_wins\"] = oscar_wins\n",
    "\n",
    "#     df_incl_exog = df_incl_exog.reset_index().merge(df_box_office_mojo, left_on=['primaryTitleFormatted', 'Year'], right_on=['Release Group', 'year'], how=\"left\").set_index('id')\n",
    "#     df_incl_exog.drop(['Release Group', 'year'], axis=1, inplace=True)\n",
    "\n",
    "#     df_incl_exog.loc[df_incl_exog['Worldwide'] == '-', 'Worldwide'] = np.nan\n",
    "#     df_incl_exog.loc[df_incl_exog['Domestic'] == '-', 'Domestic'] = np.nan\n",
    "#     df_incl_exog.loc[df_incl_exog['Foreign'] == '-', 'Foreign'] = np.nan\n",
    "#     df_incl_exog.loc[df_incl_exog['Worldwide'].notnull(), 'Worldwide'] = df_incl_exog.loc[df_incl_exog['Worldwide'].notnull(), 'Worldwide'].apply(lambda x: float(x.replace('$', '').replace(',', '')))\n",
    "#     df_incl_exog.loc[df_incl_exog['Domestic'].notnull(), 'Domestic'] = df_incl_exog.loc[df_incl_exog['Domestic'].notnull(), 'Domestic'].apply(lambda x: float(x.replace('$', '').replace(',', '')))\n",
    "#     df_incl_exog.loc[df_incl_exog['Foreign'].notnull(), 'Foreign'] = df_incl_exog.loc[df_incl_exog['Foreign'].notnull(), 'Foreign'].apply(lambda x: float(x.replace('$', '').replace(',', '')))\n",
    "\n",
    "\n",
    "#     df_incl_exog = create_remake_column(df_incl_exog)\n",
    "\n",
    "#     # # add the language of the original title, currently commented for training data usage and not wait 15 min every time\n",
    "#     # df_incl_exog = add_language_of_original_title(df_incl_exog)\n",
    "\n",
    "#     df_added_lang = pd.read_csv('additional_data/df_added_lang.csv', index_col=0)\n",
    "#     df_added_lang = df_added_lang.rename({\"tconst\" : \"id\"}, axis = 1).set_index(\"id\")\n",
    "#     df_incl_exog = df_incl_exog.join(df_added_lang['title_language'], how='left')\n",
    "\n",
    "#     df_incl_exog = add_english_title_or_not(df_incl_exog)\n",
    "#     df_incl_exog = add_movie_genre(df_incl_exog)\n",
    "#     df_incl_exog = pd.concat([df_incl_exog.T, written_and_directed.T]).T\n",
    "#     df_incl_exog.to_csv(f\"{filename}_df_with_features.csv\")\n",
    "    \n",
    "#     return df_incl_exog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19a7ad9",
   "metadata": {},
   "source": [
    "# Evaluating classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140b9241",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgbm = lgb.LGBMClassifier(objective='binary',\n",
    "                                learning_rate=0.01,\n",
    "                                num_iterations=1000,\n",
    "                                feature_fraction=0.8,\n",
    "                                verbosity=1,\n",
    "                                random_state=17)\n",
    "model_lgbm.fit(train_df_prepped.loc[:, train_df_prepped.columns != 'label'],\n",
    "              train_df_prepped['label'],\n",
    "              eval_metric='logloss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f11fafd",
   "metadata": {},
   "source": [
    "# Predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872fe110",
   "metadata": {},
   "source": [
    "## Add and process train and valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9118154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = df_processor_enrichment('validation_hidden.csv')\n",
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88c5a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df_prepped = df_model_prep(valid_df, 'valid')\n",
    "valid_df_prepped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee9186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df_processor_enrichment('test_hidden.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49bf212",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_prepped = df_model_prep(test_df, 'test')\n",
    "test_df_prepped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12194476",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds_lgbm = model_lgbm.predict(valid_df_prepped)\n",
    "test_preds_lgbm = model_lgbm.predict(test_df_prepped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461be441",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('val_preds_lgbm.txt', 'w+') as f:\n",
    "    for val in val_preds_lgbm:\n",
    "        f.write(f\"{str(val)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c5a6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_preds_lgbm.txt', 'w+') as f:\n",
    "    for val in test_preds_lgbm:\n",
    "        f.write(f\"{str(val)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
