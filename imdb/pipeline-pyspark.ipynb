{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e3098bd",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c455ad-e0be-4c3b-b77c-8aadebb25c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langdetect gensim nltk lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c41d1c-db5d-476a-8081-3961a36815b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ast\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import groupby\n",
    "\n",
    "from py_files.writer_director_to_one_hot import writer_director_to_one_hot\n",
    "from py_files.add_merge_begin_end_year import merge_start_end_year\n",
    "from py_files.load_box_office_data import load_and_aggregate_box_office\n",
    "from py_files.add_remake_feature import create_remake_column\n",
    "from py_files.add_langoriginaltitle_feature import add_language_of_original_title\n",
    "from py_files.add_ENvsNonEN_feature import add_english_title_or_not\n",
    "from py_files.add_movie_genre_feature import add_movie_genre\n",
    "from py_files.df_processor_enrichment import df_processor_enrichment\n",
    "\n",
    "from py_files.df_model_prep import df_model_prep\n",
    "from py_files.d2v_embed import d2v_embed\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "import os\n",
    "\n",
    "from pyspark.sql.functions import input_file_name, substring, udf,col, lit, coalesce,\\\n",
    "                                  when, regexp_replace, count, regexp_extract, split,\\\n",
    "                                  array_contains, monotonically_increasing_id, concat, concat_ws\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, ArrayType, FloatType\n",
    "from pyspark import SparkContext\n",
    "\n",
    "\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7152c071",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55e0256-9579-4c92-8049-e8c7d5de8f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.driver.bindAddress\",\"127.0.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '12g')\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "\n",
    "all_files = glob.glob(\"train-*.csv\")\n",
    "\n",
    "print(f\"Found files: {', '.join(all_files)}\")\n",
    "\n",
    "schema = StructType() \\\n",
    "      .add(\"_c0\",IntegerType(),True) \\\n",
    "      .add(\"tconst\",StringType(),True) \\\n",
    "      .add(\"primaryTitle\",StringType(),True) \\\n",
    "      .add(\"originalTitle\",StringType(),True) \\\n",
    "      .add(\"startYear\",IntegerType(),True) \\\n",
    "      .add(\"endYear\",IntegerType(),True) \\\n",
    "      .add(\"runtimeMinutes\",IntegerType(),True) \\\n",
    "      .add(\"numVotes\",IntegerType(),True) \\\n",
    "      .add(\"label\",BooleanType(),True)\n",
    "\n",
    "# skip the header and define our own because the automatic detection doesn't go right\n",
    "n_skip_rows = 1\n",
    "row_rdd = spark.sparkContext \\\n",
    "    .textFile(\"train-*.csv\") \\\n",
    "    .zipWithIndex() \\\n",
    "    .filter(lambda row: row[1] >= n_skip_rows) \\\n",
    "    .map(lambda row: row[0])\n",
    "\n",
    "training_data = spark.read.csv(row_rdd, schema=schema, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11d8d99-72f3-4bba-a70a-3f0148489073",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba87546d",
   "metadata": {},
   "source": [
    "# Preprocessing of original columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b24c4d8-6ad3-4170-8e48-77bc8f0cedb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_titles(title):\n",
    "    return unicodedata.normalize('NFKD',title.lower()).encode('ascii', errors='ignore').decode('utf-8').replace(\"\\W\", \"\")\n",
    "\n",
    "udf_format_titles = udf(format_titles, StringType()) # if the function returns an int\n",
    "\n",
    "training_data.show()\n",
    "training_data = training_data.withColumn(\"primaryTitleFormatted\", lit(udf_format_titles('primaryTitle')))\n",
    "training_data = training_data.withColumn('Year', coalesce('startYear', 'endYear'))\n",
    "training_data = training_data.where(col(\"tconst\") != \"tconst\")\n",
    "training_data.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3627c6f5",
   "metadata": {},
   "source": [
    "## Preprocessing of exogenous data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494afa9b",
   "metadata": {},
   "source": [
    "### Oscar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c7a2bf-5bb1-49d4-b8a4-c4ec4a4634e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "oscars = spark.read.csv(\"additional_data/oscars.csv\", header=True)\n",
    "oscars = oscars.na.drop(subset=[\"film\"])\n",
    "oscars = oscars.withColumn(\"film\", lit(udf_format_titles('film')))\n",
    "\n",
    "cond = [training_data.primaryTitleFormatted == oscars.film]\n",
    "oscar_noms = training_data.join(oscars, cond, 'inner').groupBy('tconst').count()\n",
    "oscar_wins = training_data.join(oscars, cond, 'inner').filter(col('winner') == True).groupBy('tconst').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d402b1-b523-454f-94cd-87999d7394af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oscar_noms.show()\n",
    "# oscar_wins.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20072838",
   "metadata": {},
   "source": [
    "### Razzie data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93770a78-6da9-4dac-ac4e-a6302de9a857",
   "metadata": {},
   "outputs": [],
   "source": [
    "razzies = spark.read.csv(\"additional_data/Razzies.csv\", header=True)\n",
    "razzies = razzies.na.drop(subset=[\"moviename\"])\n",
    "razzies = razzies.withColumn(\"moviename\", lit(udf_format_titles('moviename')))\n",
    "\n",
    "cond = [training_data.primaryTitleFormatted == razzies.moviename]\n",
    "razzie_noms = training_data.join(razzies, cond, 'inner').groupBy('tconst').count()\n",
    "razzie_wins = training_data.join(razzies, cond, 'inner').filter(col('Wins') == True).groupBy('tconst').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f0ef25-1e08-4900-9cc5-7ed06fd76358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# razzie_noms.show()\n",
    "# razzie_wins.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19829c71",
   "metadata": {},
   "source": [
    "### Writer and Director data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d3b6d0-4ab3-4ed1-b499-45e9b98d0ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writers = writer_director_to_one_hot(\"writers\")\n",
    "# directors = writer_director_to_one_hot(\"directors\")\n",
    "# written_and_directed = writers.add(directors, fill_value=0).fillna(0).astype(int).loc[df_preprocessed[\"tconst\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d2a0f4",
   "metadata": {},
   "source": [
    "### TMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb9da1b-afb3-47c9-8722-ec15e2f488b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "      .add(\"id\",IntegerType(),True) \\\n",
    "      .add(\"belongs_to_collection\",StringType(),True) \\\n",
    "      .add(\"budget\",IntegerType(),True) \\\n",
    "      .add(\"genres\",StringType(),True) \\\n",
    "      .add(\"homepage\",StringType(),True) \\\n",
    "      .add(\"imdb_id\",StringType(),True) \\\n",
    "      .add(\"original_language\",StringType(),True) \\\n",
    "      .add(\"original_title\",StringType(),True) \\\n",
    "      .add(\"overview\",StringType(),True) \\\n",
    "      .add(\"popularity\",FloatType(),True) \\\n",
    "      .add(\"poster_page\",StringType(),True) \\\n",
    "      .add(\"production_companies\",StringType(),True) \\\n",
    "      .add(\"production_countries\",StringType(),True) \\\n",
    "      .add(\"release_data\",StringType(),True) \\\n",
    "      .add(\"runtime\",IntegerType(),True) \\\n",
    "      .add(\"spoken_language\",StringType(),True) \\\n",
    "      .add(\"status\",StringType(),True) \\\n",
    "      .add(\"tagline\",StringType(),True) \\\n",
    "      .add(\"title\",StringType(),True) \\\n",
    "      .add(\"Keywords\",StringType(),True) \\\n",
    "      .add(\"cast\",StringType(),True) \\\n",
    "      .add(\"crew\",StringType(),True) \\\n",
    "      .add(\"revenue\",IntegerType(),True)\n",
    "\n",
    "n_skip_rows = 1\n",
    "row_rdd = spark.sparkContext \\\n",
    "    .textFile(\"additional_data/TMDB.csv\") \\\n",
    "    .zipWithIndex() \\\n",
    "    .filter(lambda row: row[1] >= n_skip_rows) \\\n",
    "    .map(lambda row: row[0])\n",
    "\n",
    "df_TMDB = spark.read.csv(row_rdd, header=False, quote='\"', escape=\"\\\"\", schema=schema).select(\"budget\", \"genres\", \"imdb_id\", \n",
    "                                                                             \"original_language\", \"overview\", \n",
    "                                                                             \"popularity\", \"production_companies\", \n",
    "                                                                             \"tagline\", \"Keywords\", \"revenue\")\n",
    "\n",
    "# # I think there are some incorrect rows present due to loading errors. \n",
    "# df_TMDB = spark.read.csv(\"additional_data/TMDB.csv\", header=True, escape=\"\\\"\")[[\"budget\", \"genres\", \"imdb_id\", \n",
    "#                                                                                 \"original_language\", \"overview\", \n",
    "#                                                                                 \"popularity\", \"production_companies\", \n",
    "#                                                                                 \"tagline\", \"Keywords\", \"revenue\"]]\n",
    "\n",
    "ids = training_data.select(\"tconst\").collect()\n",
    "ids = [i[0] for i in ids]\n",
    "df_TMDB = df_TMDB.where(col(\"imdb_id\").isin(set(ids)))\n",
    "\n",
    "df_TMDB.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebda68e6-1946-426d-9438-c8611e088f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_string(dictionary):\n",
    "    try:\n",
    "        d = ast.literal_eval(dictionary)\n",
    "    except ValueError:\n",
    "        return \"\"\n",
    "                \n",
    "    try:\n",
    "        return \" \".join([i[\"name\"] for i in d])\n",
    "    except TypeError:\n",
    "        return \"\"\n",
    "\n",
    "udf_dict_to_string = udf(lambda x: dict_to_string(x), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91a34fa-b9be-42c1-906c-07ebe582a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TMDB = df_TMDB.withColumn(\"genres\", udf_dict_to_string(col(\"genres\")))\n",
    "df_TMDB = df_TMDB.withColumn(\"Keywords\", udf_dict_to_string(col(\"Keywords\")))\n",
    "df_TMDB = df_TMDB.withColumn(\"production_companies\", udf_dict_to_string(col(\"production_companies\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b736e978",
   "metadata": {},
   "source": [
    "### Metacritic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d23e131-6a6c-4a89-bedf-a919b93cd221",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema2 = StructType() \\\n",
    "      .add(\"_c0\",IntegerType(),True) \\\n",
    "      .add(\"tconst\",StringType(),True) \\\n",
    "      .add(\"genres\",StringType(),True) \\\n",
    "      .add(\"language\",StringType(),True) \\\n",
    "      .add(\"overview\",StringType(),True) \\\n",
    "\n",
    "n_skip_rows = 1\n",
    "row_rdd2 = spark.sparkContext \\\n",
    "    .textFile(\"additional_data/Metacritic.csv\") \\\n",
    "    .zipWithIndex() \\\n",
    "    .filter(lambda row: row[1] >= n_skip_rows) \\\n",
    "    .map(lambda row: row[0])\n",
    "\n",
    "df_meta = spark.read.csv(row_rdd2, header=False, escape=\"\\\"\", schema=schema2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e44eaa-91c8-4db6-aeec-c2fb2f4c2393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_overview(overview):\n",
    "    overview = eval(overview)\n",
    "        \n",
    "    if overview:\n",
    "        return overview[0]\n",
    "    else:\n",
    "        return \" \"\n",
    "    \n",
    "udf_generate_overview = udf(lambda x: generate_overview(x), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e51bc86-1f74-4797-b4b6-36c285f9b3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = df_meta.withColumn(\"overview_meta\", udf_generate_overview(col(\"overview\"))).drop(\"overview\")\n",
    "df_meta = df_meta.withColumnRenamed(\"genres\", \"genres_meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c522aaef-5a96-40af-ac53-54a994a07e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = [df_meta.tconst == df_TMDB.imdb_id]\n",
    "full = df_meta.join(df_TMDB, cond, 'outer')\n",
    "\n",
    "overviews = full.select(concat_ws(\" \", full.overview, \n",
    "                                       full.overview_meta, \n",
    "                                       full.Keywords).alias(\"FullOverview\"), \"imdb_id\", \"tconst\", \n",
    "                                                                             \"budget\", \"genres\", \"genres_meta\", \n",
    "                                                                             \"language\", \"popularity\", \"revenue\")\n",
    "\n",
    "overviews = overviews.withColumn('movie_id', coalesce('tconst', 'imdb_id')).drop(\"tconst\", \"imdb_id\")\n",
    "overviews = overviews.where(col(\"movie_id\").isin(set(ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391dd9e1-a877-4564-b16d-62f49fcbdbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = overviews.select(\"movie_id\", \"FullOverview\").join(training_data.select(\"tconst\", \"primaryTitleFormatted\"),\n",
    "                                                          overviews[\"movie_id\"] == training_data[\"tconst\"], \n",
    "                                                          how=\"outer\").drop(\"movie_id\")\n",
    "\n",
    "texts = texts.select(\"tconst\", concat_ws(\": \", \n",
    "                                         texts.primaryTitleFormatted,\n",
    "                                         texts.FullOverview).alias(\"FullText\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd76dd6-cddc-4b5d-a027-2f0a504d046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_TMDB = overviews.drop(\"FullOverview\", \"imdb_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6131e00c-f637-4868-a0d6-ea379b6d5753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return unicodedata.normalize('NFKD', text.lower()).encode('ascii', errors='ignore').decode('utf-8').replace(\"\\W\", \"\").replace(\"'\", \"\").replace('\"', \"\")\n",
    "        \n",
    "udf_clean_text = udf(lambda x: clean_text(x), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bab7a8-8eae-4562-9fcc-c3d11adf5108",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = texts.withColumn(\"FullText\", udf_clean_text(col(\"FullText\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf8cf63-3663-44be-9a2a-5e4d94f80672",
   "metadata": {},
   "outputs": [],
   "source": [
    "overviews.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cd7c22",
   "metadata": {},
   "source": [
    "### Box Office data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f1b374-02ab-4d0c-a6c4-84c53f219744",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_office_schema = StructType() \\\n",
    "      .add(\"Rank\",IntegerType(),True) \\\n",
    "      .add(\"Release Group\",StringType(),True) \\\n",
    "      .add(\"Worldwide\",StringType(),True) \\\n",
    "      .add(\"Domestic\",StringType(),True) \\\n",
    "      .add(\"Col_to_Drop1\",StringType(),True) \\\n",
    "      .add(\"Foreign\",StringType(),True) \\\n",
    "      .add(\"Col_to_Drop2\",StringType(),True)\n",
    "\n",
    "n_skip_rows = 1\n",
    "box_office_rdd = spark.sparkContext \\\n",
    "    .textFile(\"box_office_mojo/*.csv\") \\\n",
    "    .zipWithIndex() \\\n",
    "    .filter(lambda row: row[1] >= n_skip_rows) \\\n",
    "    .map(lambda row: row[0])\n",
    "\n",
    "df_box_office_mojo = spark.read.csv(box_office_rdd, schema=box_office_schema, header=False)\n",
    "\n",
    "# process the 'release group' (read movie title) in the same way as the formatted title\n",
    "df_box_office_mojo = df_box_office_mojo.withColumn(\"Release Group\", lit(udf_format_titles('Release Group')))\n",
    "\n",
    "# add the year of the box office file\n",
    "df_box_office_mojo = df_box_office_mojo.withColumn(\"year\", substring(input_file_name(), -8, 4).cast(IntegerType()))\n",
    "\n",
    "# drop unnecessary columns\n",
    "df_box_office_mojo = df_box_office_mojo.drop(*('Col_to_Drop1', 'Col_to_Drop2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501d2746",
   "metadata": {},
   "source": [
    "# Adding of exogenous columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052d6fbf-96d9-4bfc-b257-d093dd111ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_incl_exog = df_preprocessed.copy(deep=True)\n",
    "# df_incl_exog = df_incl_exog.rename({\"tconst\" : \"id\"}, axis = 1).set_index(\"id\")\n",
    "# df_incl_exog.info()\n",
    "# training_data = training_data.withColumnRenamed('tconst', 'id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bd06e1",
   "metadata": {},
   "source": [
    "## add oscar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f96bdf0-384e-4869-96a3-0c7314a973da",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.join(oscar_noms, ['tconst'], 'left').withColumnRenamed('count', 'oscar_noms')\n",
    "training_data = training_data.join(oscar_wins, ['tconst'], 'left').withColumnRenamed('count', 'oscar_wins')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a84eb1c",
   "metadata": {},
   "source": [
    "## add razzie data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c66a93-98ba-4426-aff8-2b825d95f0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.join(razzie_noms, ['tconst'], 'left').withColumnRenamed('count', 'razzie_noms')\n",
    "training_data = training_data.join(razzie_wins, ['tconst'], 'left').withColumnRenamed('count', 'razzie_wins')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c17320-e41c-40a1-a9b1-2f9ddb04c8ee",
   "metadata": {},
   "source": [
    "## add TMDB & Metacritic data\n",
    "\n",
    "THIS DOES NOT INCLUDE THE OVERVIEWS, THEY WILL BE ADDED LATER, AFTER BEING CONVERTED TO D2V!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493c67d8-15fd-4f5a-be96-e2db5e611954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using overviews2, since overviews causes memory issues\n",
    "cond = [training_data.tconst == meta_TMDB.tconst]\n",
    "\n",
    "training_data = training_data.join(meta_TMDB, cond, \"leftouter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9105936a-7847-4381-82e2-109de7d7e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac09b811",
   "metadata": {},
   "source": [
    "## add mojo box office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf2928b-b2f7-42e4-b23b-24ac8608b7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missing_box_office_values(column):\n",
    "    return when(column != '-', column).otherwise(lit(None))\n",
    "\n",
    "cond_mojo_merge = [training_data.primaryTitleFormatted == df_box_office_mojo['Release Group'], training_data.Year == df_box_office_mojo.year]\n",
    "\n",
    "training_data = training_data.join(df_box_office_mojo, cond_mojo_merge, 'left').drop(*('Release Group', 'year'))\n",
    "training_data = training_data.withColumn(\"Worldwide\", remove_missing_box_office_values(col(\"Worldwide\")))\n",
    "training_data = training_data.withColumn(\"Domestic\", remove_missing_box_office_values(col(\"Domestic\")))\n",
    "training_data = training_data.withColumn(\"Foreign\", remove_missing_box_office_values(col(\"Foreign\")))\n",
    "training_data = training_data.withColumn('Worldwide', regexp_replace('Worldwide', '[$,]', '').cast('double'))\n",
    "training_data = training_data.withColumn('Domestic', regexp_replace('Domestic', '[$,]', '').cast('double'))\n",
    "training_data = training_data.withColumn('Foreign', regexp_replace('Foreign', '[$,]', '').cast('double'))\n",
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264c2789",
   "metadata": {},
   "source": [
    "## add remake column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a232b91c-fa98-4458-be27-404e008ebf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.join(\n",
    "    training_data.groupBy(\"primaryTitle\").agg((count(\"*\")>1).cast(\"int\").alias(\"hasRemake\")),\n",
    "    on=\"primaryTitle\",\n",
    "    how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec691609",
   "metadata": {},
   "source": [
    "## add title language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c2e168-adec-402e-bdaf-54ec9ffaf697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add the language of the original title, currently commented for training data usage and not wait 15 min every time\n",
    "# df_incl_exog = add_language_of_original_title(df_incl_exog)\n",
    "\n",
    "# df_added_lang = pd.read_csv('additional_data/df_added_lang.csv', index_col=0)\n",
    "# df_added_lang = df_added_lang.rename({\"tconst\" : \"id\"}, axis = 1).set_index(\"id\")\n",
    "# df_incl_exog = df_incl_exog.join(df_added_lang['title_language'], how='left')\n",
    "\n",
    "added_lang_schema = StructType() \\\n",
    "      .add(\"_c0\",IntegerType(),True) \\\n",
    "      .add(\"tconst\",StringType(),True) \\\n",
    "      .add(\"primaryTitle\",StringType(),True) \\\n",
    "      .add(\"originalTitle\",StringType(),True) \\\n",
    "      .add(\"startYear\",IntegerType(),True) \\\n",
    "      .add(\"endYear\",IntegerType(),True) \\\n",
    "      .add(\"runtimeMinutes\",IntegerType(),True) \\\n",
    "      .add(\"numVotes\",IntegerType(),True) \\\n",
    "      .add(\"label\",BooleanType(),True) \\\n",
    "      .add(\"title_language\",StringType(),True) \\\n",
    "      .add(\"isEN\",BooleanType(),True) \n",
    "\n",
    "n_skip_rows = 1\n",
    "added_lang_rdd = spark.sparkContext \\\n",
    "    .textFile('additional_data/df_added_lang.csv') \\\n",
    "    .zipWithIndex() \\\n",
    "    .filter(lambda row: row[1] >= n_skip_rows) \\\n",
    "    .map(lambda row: row[0])\n",
    "\n",
    "df_added_lang = spark.read.csv(added_lang_rdd, schema=added_lang_schema, header=False)\n",
    "\n",
    "training_data = training_data.join(df_added_lang.select(['tconst', 'title_language']), on='tconst', how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa17831",
   "metadata": {},
   "source": [
    "## add whether title is English or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3bab01-c3f6-4188-b4e1-6924a70a7a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indicate_whether_language_is_english(column):\n",
    "    return when(column == 'en', True).otherwise(lit(False))\n",
    "\n",
    "training_data = training_data.withColumn(\"isEN\", indicate_whether_language_is_english(col(\"title_language\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bc1096-ae04-40a7-8367-4a1fe9b279a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a68ab6",
   "metadata": {},
   "source": [
    "## add movie genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deae718-3950-472c-9973-29fbe3dedf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_incl_exog = add_movie_genre(df_incl_exog)\n",
    "\n",
    "def retrieve_year(string):\n",
    "    try:\n",
    "        return int(re.search('\\((.*?)\\)', string).group()[1:-1])\n",
    "    except:\n",
    "        return pd.NA\n",
    "\n",
    "def remove_year(string):\n",
    "    try:\n",
    "        return re.sub('\\((.*?)\\)', '', string)[:-1]\n",
    "    except:\n",
    "        return str\n",
    "\n",
    "def add_movie_genre(df_):\n",
    "    ''''Create onehot encoded features of genres'''\n",
    "    \n",
    "    # load movies with genre data\n",
    "    movie_genres = pd.read_csv(r'additional_data/movie_genres.csv', index_col=0)\n",
    "\n",
    "    # remove movies in data set that don't have genres\n",
    "    movie_genres = movie_genres[movie_genres['genres'] != '(no genres listed)']\n",
    "    \n",
    "    # get date for each movie from title column\n",
    "    movie_genres['year'] = movie_genres['title'].apply(lambda x: retrieve_year(x))\n",
    "    movie_genres = movie_genres.dropna(subset=['year'])\n",
    "\n",
    "    # remove year from title column and set title data type correctly\n",
    "    movie_genres['year'] = movie_genres['year'].astype(int)\n",
    "    movie_genres['title'] = movie_genres['title'].apply(lambda x: remove_year(x)).astype('string')\n",
    "    movie_genres['genres'] = movie_genres['genres'].apply(lambda x: x.split('|'))\n",
    "    \n",
    "    # format title in same way as original dataset\n",
    "    movie_genres[\"titleFormatted\"] = movie_genres[\"title\"].str.lower()\\\n",
    "                                       .str.normalize('NFKD')\\\n",
    "                                       .str.encode('ascii', errors='ignore')\\\n",
    "                                       .str.decode('utf-8')\\\n",
    "                                       .str.replace(\" \", \"_\", regex=True)\\\n",
    "                                       .str.replace(\"\\W\", \"\", regex=True)\n",
    "    \n",
    "    movie_genres.drop_duplicates(subset=['titleFormatted', 'year'], inplace=True)\n",
    "    \n",
    "    df_ = df_.reset_index().merge(movie_genres[['year', 'titleFormatted', 'genres']], left_on=['primaryTitleFormatted', 'Year'], right_on=['titleFormatted', 'year'], how='left').set_index('id')\n",
    "    s = df_['genres'].explode()\n",
    "    df_ = df_.join(pd.crosstab(s.index, s), how='left')\n",
    "    \n",
    "    return df_\n",
    "    \n",
    "movie_genres = spark.read.csv(\"additional_data/movie_genres.csv\", header=True)\n",
    "movie_genres = movie_genres.filter(movie_genres.genres != '(no genres listed)')\n",
    "movie_genres = movie_genres.filter(movie_genres.title.endswith(')'))\n",
    "movie_genres = movie_genres.withColumn('year', substring(col('title'), -5, 4))\n",
    "movie_genres = movie_genres.filter(movie_genres.year != '')\n",
    "movie_genres = movie_genres.withColumn('year', col('year').cast(IntegerType()))\n",
    "movie_genres = movie_genres.withColumn('title', regexp_replace(col('title'), r' \\(.*?\\)', ''))\n",
    "movie_genres = movie_genres.withColumn('genres', split(col('genres'), '\\|'))\n",
    "movie_genres = movie_genres.withColumn(\"titleFormatted\", lit(udf_format_titles('title')))\n",
    "movie_genres = movie_genres.dropDuplicates(['titleFormatted', 'year'])\n",
    "\n",
    "list_of_genres = ['Action', 'Adventure', 'Animation', 'Children', 'Comedy',\n",
    "       'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',\n",
    "       'IMAX', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War',\n",
    "       'Western']\n",
    "\n",
    "for c in list_of_genres:\n",
    "    movie_genres = movie_genres.withColumn(c, array_contains(\"genres\", c).cast(\"int\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f5c2b1",
   "metadata": {},
   "source": [
    "## add writers and directors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc7cde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incl_exog = pd.concat([df_incl_exog.T, written_and_directed.T]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206a74ec",
   "metadata": {},
   "source": [
    "## save dataframe with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e090e7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incl_exog.to_csv('df_with_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eedb0b6",
   "metadata": {},
   "source": [
    "# Preparing data for classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1874804",
   "metadata": {},
   "source": [
    "Convert non-numeric columns to numeric.\n",
    "We use Doc2Vec to embed each string column into n-by-128 array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c1330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('df_with_features.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c9da0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_prepped = df_model_prep(train_df,'train')\n",
    "train_df_prepped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d8449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_model_prep function for demonstration purposes\n",
    "#\n",
    "# from py_files.d2v_embed import d2v_embed\n",
    "# import pandas as pd\n",
    "# import math\n",
    "\n",
    "# def df_model_prep(df, filename):\n",
    "    \n",
    "#     try:\n",
    "#         print(\"Looking for pre made file...\")\n",
    "#         return pd.read_csv(f\"{filename}_df_with_features_fully_processed_read_for_model.csv\", index_col = 0)\n",
    "#     except:\n",
    "#         print(\"No file found, creating a new one\")\n",
    "    \n",
    "#     prim_title_df = d2v_embed(df['primaryTitle'])\n",
    "#     orig_title_df = d2v_embed(df['originalTitle'])\n",
    "#     prim_title_formatted_df = d2v_embed(df['primaryTitleFormatted'])\n",
    "#     title_formatted_df = d2v_embed(df['titleFormatted'])\n",
    "#     genres_df = d2v_embed(df['genres'])\n",
    "\n",
    "#     # just encode languages into ints for this column\n",
    "#     df['title_language'] = pd.factorize(df['title_language'])[0]\n",
    "\n",
    "#     df.drop(columns = df.select_dtypes(include='object').columns, inplace=True)\n",
    "\n",
    "#     # dealing with (some) nan values\n",
    "#     for index, row in df.iterrows():\n",
    "#         # For missing startYear or endYear entries, insert the other, if it exists.\n",
    "#         if math.isnan(row['startYear']):\n",
    "#             if not math.isnan(row['endYear']):\n",
    "#                 df.at[index,'startYear']=df.at[index,'endYear']\n",
    "#         if math.isnan(row['endYear']):\n",
    "#             if not math.isnan(row['startYear']):\n",
    "#                 df.at[index,'endYear']=df.at[index,'startYear']\n",
    "\n",
    "#         # For missing oscar_noms and oscar_wins, insert 0\n",
    "#         if math.isnan(row['oscar_noms']):\n",
    "#             df.at[index,'oscar_noms'] = 0\n",
    "#         if math.isnan(row['oscar_wins']):\n",
    "#             df.at[index,'oscar_wins'] = 0\n",
    "#         if math.isnan(row['razzie_noms']):\n",
    "#             df.at[index,'razzie_noms'] = 0\n",
    "#         if math.isnan(row['razzie_wins']):\n",
    "#             df.at[index,'razzie_wins'] = 0\n",
    "\n",
    "#     df['numVotes'] = df['numVotes'].fillna(df['numVotes'].mean(skipna=True))\n",
    "#     df['runtimeMinutes'] = df['runtimeMinutes'].fillna(df['runtimeMinutes'].mean(skipna=True))\n",
    "    \n",
    "#     df['title_language'] = pd.factorize(df['title_language'])[0]\n",
    "    \n",
    "#     df = df.join(prim_title_df)\n",
    "#     df = df.join(orig_title_df)\n",
    "#     df = df.join(prim_title_formatted_df)\n",
    "#     df = df.join(title_formatted_df)\n",
    "#     df = df.join(genres_df)\n",
    "    \n",
    "#     df.to_csv(f\"{filename}_df_with_features_fully_processed_read_for_model.csv\")\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc54e3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d2v_embed function for demonstration purposes\n",
    "# \n",
    "# from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# import multiprocessing as mp\n",
    "# from tqdm import tqdm\n",
    "# import pandas as pd\n",
    "# import math\n",
    "\n",
    "# def d2v_embed(df_col, max_epochs = 100, vec_size = 128, alpha = 0.025):\n",
    "    \n",
    "#     df_col = df_col.fillna(\" \")\n",
    "#     df_col = df_col.str.lower()\\\n",
    "#                    .str.normalize('NFKD')\\\n",
    "#                    .str.encode('ascii', errors='ignore')\\\n",
    "#                    .str.decode('utf-8')\\\n",
    "#                    .str.replace(\"\\W\", \" \", regex=True)\n",
    "    \n",
    "#     tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(df_col)]\n",
    "\n",
    "#     model = Doc2Vec(vector_size=vec_size,\n",
    "#                     alpha=alpha, \n",
    "#                     min_alpha=0.00025,\n",
    "#                     min_count=1,\n",
    "#                     dm =1,\n",
    "#                     workers = mp.cpu_count())\n",
    "  \n",
    "#     model.build_vocab(tagged_data)\n",
    "\n",
    "#     for epoch in tqdm(range(max_epochs)):\n",
    "#     #     print('iteration {0}'.format(epoch))\n",
    "#         model.train(tagged_data,\n",
    "#                     total_examples=model.corpus_count,\n",
    "#                     epochs=model.epochs)\n",
    "#         # decrease the learning rate\n",
    "#         model.alpha -= 0.0002\n",
    "#         # fix the learning rate, no decay\n",
    "#         model.min_alpha = model.alpha\n",
    "    \n",
    "#     # save model\n",
    "#     model.save(f\"doc2vec_model_{df_col.name}.model\")\n",
    "    \n",
    "#     #return df with doc embeddings\n",
    "#     return pd.DataFrame([model.docvecs[i] for i in range(len(df_col))], \n",
    "#                         index = df_col.index,\n",
    "#                         columns = [f\"{df_col.name}_{i}\" for i in range(vec_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a5e571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_processor_enrichment function for demonstration purposes\n",
    "# \n",
    "# import json\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from itertools import groupby\n",
    "\n",
    "# from py_files.writer_director_to_one_hot import writer_director_to_one_hot\n",
    "# from py_files.add_merge_begin_end_year import merge_start_end_year\n",
    "# from py_files.load_box_office_data import load_and_aggregate_box_office\n",
    "# from py_files.add_remake_feature import create_remake_column\n",
    "# from py_files.add_langoriginaltitle_feature import add_language_of_original_title\n",
    "# from py_files.add_ENvsNonEN_feature import add_english_title_or_not\n",
    "# from py_files.add_movie_genre_feature import add_movie_genre\n",
    "\n",
    "# from py_files.d2v_embed import d2v_embed\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import lightgbm as lgb\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# import math\n",
    "\n",
    "# def df_processor_enrichment(filename):\n",
    "    \n",
    "#     try:\n",
    "#         print(\"Looking for pre made file...\")\n",
    "#         return pd.read_csv(f\"{filename}_df_with_features.csv\", index_col = 0)\n",
    "#     except:\n",
    "#         print(\"File not found, creating a new one..\")\n",
    "              \n",
    "#     df_original = pd.read_csv(filename, index_col=0)\n",
    "#     # df_original.head()\n",
    "\n",
    "#     # start the preprocessing\n",
    "#     df_preprocessed = df_original.replace(\"\\\\N\", np.nan)\n",
    "#     df_preprocessed[\"primaryTitleFormatted\"] = df_preprocessed[\"primaryTitle\"].str.lower()\\\n",
    "#                                                                               .str.normalize('NFKD')\\\n",
    "#                                                                               .str.encode('ascii', errors='ignore')\\\n",
    "#                                                                               .str.decode('utf-8')\\\n",
    "#                                                                               .str.replace(\" \", \"_\", regex=True)\\\n",
    "#                                                                               .str.replace(\"\\W\", \"\", regex=True)\n",
    "\n",
    "#     # merge endYear into beginYear when beginYear is not available --> rename Year\n",
    "#     df_preprocessed = merge_start_end_year(df_preprocessed)\n",
    "\n",
    "#     # set the datatypes of the dataframe correctly\n",
    "#     df_preprocessed['Year'] = df_preprocessed['Year'].astype(int)\n",
    "#     df_preprocessed['runtimeMinutes'] = df_preprocessed['runtimeMinutes'].astype(float)\n",
    "\n",
    "#     # df_preprocessed.info()\n",
    "\n",
    "\n",
    "#     oscars = pd.read_csv(\"additional_data/oscars.csv\")\n",
    "\n",
    "#     oscars[\"film\"] = oscars[\"film\"].str.lower()\\\n",
    "#                                    .str.normalize('NFKD')\\\n",
    "#                                    .str.encode('ascii', errors='ignore')\\\n",
    "#                                    .str.decode('utf-8')\\\n",
    "#                                    .str.replace(\" \", \"_\", regex=True)\\\n",
    "#                                    .str.replace(\"\\W\", \"\", regex=True)\n",
    "\n",
    "#     # Counting oscar nominations and wins per movie\n",
    "#     oscar_noms = pd.merge(df_preprocessed, oscars, left_on = \"primaryTitleFormatted\", right_on = \"film\").groupby(\"tconst\")[\"winner\"].count()\n",
    "#     oscar_wins = pd.merge(df_preprocessed, oscars, left_on = \"primaryTitleFormatted\", right_on = \"film\").groupby(\"tconst\")[\"winner\"].sum()\n",
    "\n",
    "\n",
    "#     # Find writers and directors per movie and combine the two\n",
    "#     written_and_directed = (writer_director_to_one_hot(\"writers\") + writer_director_to_one_hot(\"directors\")).fillna(0).astype(int).loc[df_preprocessed['tconst']]\n",
    "\n",
    "\n",
    "#     df_box_office_mojo = load_and_aggregate_box_office()\n",
    "\n",
    "#     # process the 'release group' (read movie title) in the same way as the formatted title\n",
    "#     df_box_office_mojo[\"Release Group\"] = df_box_office_mojo[\"Release Group\"].str.lower()\\\n",
    "#                                            .str.normalize('NFKD')\\\n",
    "#                                            .str.encode('ascii', errors='ignore')\\\n",
    "#                                            .str.decode('utf-8')\\\n",
    "#                                            .str.replace(\" \", \"_\", regex=True)\\\n",
    "#                                            .str.replace(\"\\W\", \"\", regex=True)\n",
    "#     df_box_office_mojo.drop(['%', '%.1'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "#     df_incl_exog = df_preprocessed.copy(deep=True)\n",
    "#     df_incl_exog = df_incl_exog.rename({\"tconst\" : \"id\"}, axis = 1).set_index(\"id\")\n",
    "#     # df_incl_exog.info()\n",
    "\n",
    "\n",
    "#     df_incl_exog[\"oscar_noms\"] = oscar_noms\n",
    "#     df_incl_exog[\"oscar_wins\"] = oscar_wins\n",
    "\n",
    "#     df_incl_exog = df_incl_exog.reset_index().merge(df_box_office_mojo, left_on=['primaryTitleFormatted', 'Year'], right_on=['Release Group', 'year'], how=\"left\").set_index('id')\n",
    "#     df_incl_exog.drop(['Release Group', 'year'], axis=1, inplace=True)\n",
    "\n",
    "#     df_incl_exog.loc[df_incl_exog['Worldwide'] == '-', 'Worldwide'] = np.nan\n",
    "#     df_incl_exog.loc[df_incl_exog['Domestic'] == '-', 'Domestic'] = np.nan\n",
    "#     df_incl_exog.loc[df_incl_exog['Foreign'] == '-', 'Foreign'] = np.nan\n",
    "#     df_incl_exog.loc[df_incl_exog['Worldwide'].notnull(), 'Worldwide'] = df_incl_exog.loc[df_incl_exog['Worldwide'].notnull(), 'Worldwide'].apply(lambda x: float(x.replace('$', '').replace(',', '')))\n",
    "#     df_incl_exog.loc[df_incl_exog['Domestic'].notnull(), 'Domestic'] = df_incl_exog.loc[df_incl_exog['Domestic'].notnull(), 'Domestic'].apply(lambda x: float(x.replace('$', '').replace(',', '')))\n",
    "#     df_incl_exog.loc[df_incl_exog['Foreign'].notnull(), 'Foreign'] = df_incl_exog.loc[df_incl_exog['Foreign'].notnull(), 'Foreign'].apply(lambda x: float(x.replace('$', '').replace(',', '')))\n",
    "\n",
    "\n",
    "#     df_incl_exog = create_remake_column(df_incl_exog)\n",
    "\n",
    "#     # # add the language of the original title, currently commented for training data usage and not wait 15 min every time\n",
    "#     # df_incl_exog = add_language_of_original_title(df_incl_exog)\n",
    "\n",
    "#     df_added_lang = pd.read_csv('additional_data/df_added_lang.csv', index_col=0)\n",
    "#     df_added_lang = df_added_lang.rename({\"tconst\" : \"id\"}, axis = 1).set_index(\"id\")\n",
    "#     df_incl_exog = df_incl_exog.join(df_added_lang['title_language'], how='left')\n",
    "\n",
    "#     df_incl_exog = add_english_title_or_not(df_incl_exog)\n",
    "#     df_incl_exog = add_movie_genre(df_incl_exog)\n",
    "#     df_incl_exog = pd.concat([df_incl_exog.T, written_and_directed.T]).T\n",
    "#     df_incl_exog.to_csv(f\"{filename}_df_with_features.csv\")\n",
    "    \n",
    "#     return df_incl_exog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19a7ad9",
   "metadata": {},
   "source": [
    "# Evaluating classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140b9241",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgbm = lgb.LGBMClassifier(objective='binary',\n",
    "                                learning_rate=0.01,\n",
    "                                num_iterations=1000,\n",
    "                                feature_fraction=0.8,\n",
    "                                verbosity=1,\n",
    "                                random_state=17)\n",
    "model_lgbm.fit(train_df_prepped.loc[:, train_df_prepped.columns != 'label'],\n",
    "              train_df_prepped['label'],\n",
    "              eval_metric='logloss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f11fafd",
   "metadata": {},
   "source": [
    "# Predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872fe110",
   "metadata": {},
   "source": [
    "## Add and process train and valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9118154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = df_processor_enrichment('validation_hidden.csv')\n",
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88c5a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df_prepped = df_model_prep(valid_df, 'valid')\n",
    "valid_df_prepped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee9186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df_processor_enrichment('test_hidden.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49bf212",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_prepped = df_model_prep(test_df, 'test')\n",
    "test_df_prepped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12194476",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds_lgbm = model_lgbm.predict(valid_df_prepped)\n",
    "test_preds_lgbm = model_lgbm.predict(test_df_prepped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461be441",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('val_preds_lgbm.txt', 'w+') as f:\n",
    "    for val in val_preds_lgbm:\n",
    "        f.write(f\"{str(val)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c5a6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_preds_lgbm.txt', 'w+') as f:\n",
    "    for val in test_preds_lgbm:\n",
    "        f.write(f\"{str(val)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
