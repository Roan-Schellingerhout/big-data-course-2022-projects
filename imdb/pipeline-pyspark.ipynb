{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e3098bd",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f424f2ff-addc-4c92-bba7-c9db576250da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim nltk lightgbm langdetect\n",
    "# run those in terminal\n",
    "# conda install -c conda-forge langdetect\n",
    "# conda install -c brittainhard fancyimpute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdd0dc3",
   "metadata": {},
   "source": [
    "## Notes and Current issues:\n",
    "\n",
    "- Currently, the data is exported from the spark pipeline to be used from the classifier. This happens in three files per set (training, validation, test). Example for the training set:\n",
    "    - The texts dataset is exported into csv and manually saved with name \"texts_df_train.csv\"(texts.write.mode(\"overwrite\").option(\"header\", True).csv(\"texts_train\")) \n",
    "    - The imputed columns are exported into csv and manually saved with name \"imputed_training_df.csv\"(imputed_data.write.mode(\"overwrite\").option(\"header\",True).csv(\"imputed_data\"))\n",
    "    - The training_data is exported into csv and manually saved with name \"training_df.csv\"\n",
    "    \n",
    "The aforementioned files can be found into the \"Data\" directory. \n",
    "\n",
    "Current Issues:\n",
    "\n",
    "- Jim's imputation function is not running properly and the usual retryingblock error is produced. In this version, the merging of movie genres to the training_data is (supposedly) fixed. \n",
    "    - This may be fixable if we export the one hot encoded genres to a separate file.\n",
    "    - Another (non-spark) fix could happen in the data_prep() function (see how column 'language' is one-hot encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44db378-e33a-41e8-85b8-7ea82f6315a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ast\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import groupby\n",
    "\n",
    "from py_files.writer_director_to_one_hot import writer_director_to_one_hot\n",
    "from py_files.add_merge_begin_end_year import merge_start_end_year\n",
    "from py_files.load_box_office_data import load_and_aggregate_box_office\n",
    "from py_files.add_remake_feature import create_remake_column\n",
    "from py_files.add_langoriginaltitle_feature import add_language_of_original_title\n",
    "from py_files.add_ENvsNonEN_feature import add_english_title_or_not\n",
    "from py_files.add_movie_genre_feature import add_movie_genre\n",
    "from py_files.df_processor_enrichment import df_processor_enrichment\n",
    "\n",
    "from py_files.df_model_prep import df_model_prep\n",
    "from py_files.d2v_embed import d2v_embed\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "\n",
    "from pyspark.sql.functions import input_file_name, substring, udf,col, lit, coalesce,\\\n",
    "                                  when, regexp_replace, count, regexp_extract, split,\\\n",
    "                                  array_contains, monotonically_increasing_id, concat, concat_ws\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, ArrayType, FloatType, LongType, DoubleType\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from fancyimpute import KNN, IterativeImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import unicodedata\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7152c071",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55e0256-9579-4c92-8049-e8c7d5de8f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.driver.bindAddress\",\"127.0.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '12g')\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "\n",
    "all_files = glob.glob(\"train-*.csv\")\n",
    "\n",
    "print(f\"Found files: {', '.join(all_files)}\")\n",
    "\n",
    "schema = StructType() \\\n",
    "      .add(\"_c0\",IntegerType(),True) \\\n",
    "      .add(\"tconst\",StringType(),True) \\\n",
    "      .add(\"primaryTitle\",StringType(),True) \\\n",
    "      .add(\"originalTitle\",StringType(),True) \\\n",
    "      .add(\"startYear\",IntegerType(),True) \\\n",
    "      .add(\"endYear\",IntegerType(),True) \\\n",
    "      .add(\"runtimeMinutes\",IntegerType(),True) \\\n",
    "      .add(\"numVotes\",FloatType(),True) \\\n",
    "      .add(\"label\",BooleanType(),True)\n",
    "\n",
    "# skip the header and define our own because the automatic detection doesn't go right\n",
    "n_skip_rows = 1\n",
    "row_rdd = spark.sparkContext \\\n",
    "    .textFile(\"validation_hidden.csv\") \\\n",
    "    .zipWithIndex() \\\n",
    "    .filter(lambda row: row[1] >= n_skip_rows) \\\n",
    "    .map(lambda row: row[0])\n",
    "\n",
    "training_data = spark.read.csv(row_rdd, schema=schema, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11d8d99-72f3-4bba-a70a-3f0148489073",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba87546d",
   "metadata": {},
   "source": [
    "# Preprocessing of original columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b24c4d8-6ad3-4170-8e48-77bc8f0cedb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_titles(title):\n",
    "    return unicodedata.normalize('NFKD',title.lower()).encode('ascii', errors='ignore').decode('utf-8').replace(\"\\W\", \"\")\n",
    "\n",
    "udf_format_titles = udf(format_titles, StringType()) # if the function returns an int\n",
    "\n",
    "training_data.show()\n",
    "training_data = training_data.withColumn(\"primaryTitleFormatted\", lit(udf_format_titles('primaryTitle')))\n",
    "training_data = training_data.withColumn('Year', coalesce('startYear', 'endYear'))\n",
    "training_data = training_data.where(col(\"tconst\") != \"tconst\")\n",
    "# training_data.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3627c6f5",
   "metadata": {},
   "source": [
    "## Preprocessing of exogenous data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494afa9b",
   "metadata": {},
   "source": [
    "### Oscar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c7a2bf-5bb1-49d4-b8a4-c4ec4a4634e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "oscars = spark.read.csv(\"additional_data/oscars.csv\", header=True)\n",
    "oscars = oscars.na.drop(subset=[\"film\"])\n",
    "oscars = oscars.withColumn(\"film\", lit(udf_format_titles('film')))\n",
    "\n",
    "cond = [training_data.primaryTitleFormatted == oscars.film]\n",
    "oscar_noms = training_data.join(oscars, cond, 'inner').groupBy('tconst').count()\n",
    "oscar_wins = training_data.join(oscars, cond, 'inner').filter(col('winner') == True).groupBy('tconst').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d402b1-b523-454f-94cd-87999d7394af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oscar_noms.show()\n",
    "# oscar_wins.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20072838",
   "metadata": {},
   "source": [
    "### Razzie data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93770a78-6da9-4dac-ac4e-a6302de9a857",
   "metadata": {},
   "outputs": [],
   "source": [
    "razzies = spark.read.csv(\"additional_data/Razzies.csv\", header=True)\n",
    "razzies = razzies.na.drop(subset=[\"moviename\"])\n",
    "razzies = razzies.withColumn(\"moviename\", lit(udf_format_titles('moviename')))\n",
    "\n",
    "cond = [training_data.primaryTitleFormatted == razzies.moviename]\n",
    "razzie_noms = training_data.join(razzies, cond, 'inner').groupBy('tconst').count()\n",
    "razzie_wins = training_data.join(razzies, cond, 'inner').filter(col('Wins') == True).groupBy('tconst').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f0ef25-1e08-4900-9cc5-7ed06fd76358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# razzie_noms.show()\n",
    "# razzie_wins.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19829c71",
   "metadata": {},
   "source": [
    "### Writer and Director data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d3b6d0-4ab3-4ed1-b499-45e9b98d0ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writers = writer_director_to_one_hot(\"writers\")\n",
    "# directors = writer_director_to_one_hot(\"directors\")\n",
    "# written_and_directed = writers.add(directors, fill_value=0).fillna(0).astype(int).loc[df_preprocessed[\"tconst\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d2a0f4",
   "metadata": {},
   "source": [
    "### TMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb9da1b-afb3-47c9-8722-ec15e2f488b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "      .add(\"id\",IntegerType(),True) \\\n",
    "      .add(\"belongs_to_collection\",StringType(),True) \\\n",
    "      .add(\"budget\",IntegerType(),True) \\\n",
    "      .add(\"genres\",StringType(),True) \\\n",
    "      .add(\"homepage\",StringType(),True) \\\n",
    "      .add(\"imdb_id\",StringType(),True) \\\n",
    "      .add(\"original_language\",StringType(),True) \\\n",
    "      .add(\"original_title\",StringType(),True) \\\n",
    "      .add(\"overview\",StringType(),True) \\\n",
    "      .add(\"popularity\",FloatType(),True) \\\n",
    "      .add(\"poster_page\",StringType(),True) \\\n",
    "      .add(\"production_companies\",StringType(),True) \\\n",
    "      .add(\"production_countries\",StringType(),True) \\\n",
    "      .add(\"release_data\",StringType(),True) \\\n",
    "      .add(\"runtime\",IntegerType(),True) \\\n",
    "      .add(\"spoken_language\",StringType(),True) \\\n",
    "      .add(\"status\",StringType(),True) \\\n",
    "      .add(\"tagline\",StringType(),True) \\\n",
    "      .add(\"title\",StringType(),True) \\\n",
    "      .add(\"Keywords\",StringType(),True) \\\n",
    "      .add(\"cast\",StringType(),True) \\\n",
    "      .add(\"crew\",StringType(),True) \\\n",
    "      .add(\"revenue\",IntegerType(),True)\n",
    "\n",
    "n_skip_rows = 1\n",
    "row_rdd = spark.sparkContext \\\n",
    "    .textFile(\"additional_data/TMDB.csv\") \\\n",
    "    .zipWithIndex() \\\n",
    "    .filter(lambda row: row[1] >= n_skip_rows) \\\n",
    "    .map(lambda row: row[0])\n",
    "\n",
    "df_TMDB = spark.read.csv(row_rdd, header=False, quote='\"', escape=\"\\\"\", schema=schema).select(\"budget\", \"genres\", \"imdb_id\", \n",
    "                                                                             \"original_language\", \"overview\", \n",
    "                                                                             \"popularity\", \"production_companies\", \n",
    "                                                                             \"tagline\", \"Keywords\", \"revenue\")\n",
    "\n",
    "# # I think there are some incorrect rows present due to loading errors. \n",
    "# df_TMDB = spark.read.csv(\"additional_data/TMDB.csv\", header=True, escape=\"\\\"\")[[\"budget\", \"genres\", \"imdb_id\", \n",
    "#                                                                                 \"original_language\", \"overview\", \n",
    "#                                                                                 \"popularity\", \"production_companies\", \n",
    "#                                                                                 \"tagline\", \"Keywords\", \"revenue\"]]\n",
    "\n",
    "ids = training_data.select(\"tconst\").collect()\n",
    "ids = [i[0] for i in ids]\n",
    "df_TMDB = df_TMDB.where(col(\"imdb_id\").isin(set(ids)))\n",
    "\n",
    "# df_TMDB.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebda68e6-1946-426d-9438-c8611e088f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_string(dictionary):\n",
    "    try:\n",
    "        d = ast.literal_eval(dictionary)\n",
    "    except ValueError:\n",
    "        return \"\"\n",
    "                \n",
    "    try:\n",
    "        return \" \".join([i[\"name\"] for i in d])\n",
    "    except TypeError:\n",
    "        return \"\"\n",
    "\n",
    "udf_dict_to_string = udf(lambda x: dict_to_string(x), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91a34fa-b9be-42c1-906c-07ebe582a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TMDB = df_TMDB.withColumn(\"genres\", udf_dict_to_string(col(\"genres\")))\n",
    "df_TMDB = df_TMDB.withColumn(\"Keywords\", udf_dict_to_string(col(\"Keywords\")))\n",
    "df_TMDB = df_TMDB.withColumn(\"production_companies\", udf_dict_to_string(col(\"production_companies\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b736e978",
   "metadata": {},
   "source": [
    "### Metacritic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d23e131-6a6c-4a89-bedf-a919b93cd221",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema2 = StructType() \\\n",
    "      .add(\"_c0\",IntegerType(),True) \\\n",
    "      .add(\"tconst\",StringType(),True) \\\n",
    "      .add(\"genres\",StringType(),True) \\\n",
    "      .add(\"language\",StringType(),True) \\\n",
    "      .add(\"overview\",StringType(),True) \\\n",
    "\n",
    "n_skip_rows = 1\n",
    "row_rdd2 = spark.sparkContext \\\n",
    "    .textFile(\"additional_data/Metacritic_validation.csv\") \\\n",
    "    .zipWithIndex() \\\n",
    "    .filter(lambda row: row[1] >= n_skip_rows) \\\n",
    "    .map(lambda row: row[0])\n",
    "\n",
    "df_meta = spark.read.csv(row_rdd2, header=False, escape=\"\\\"\", schema=schema2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e44eaa-91c8-4db6-aeec-c2fb2f4c2393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_overview(overview):\n",
    "    overview = eval(overview)\n",
    "        \n",
    "    if overview:\n",
    "        return overview[0]\n",
    "    else:\n",
    "        return \" \"\n",
    "    \n",
    "udf_generate_overview = udf(lambda x: generate_overview(x), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e51bc86-1f74-4797-b4b6-36c285f9b3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = df_meta.withColumn(\"overview_meta\", udf_generate_overview(col(\"overview\"))).drop(\"overview\")\n",
    "df_meta = df_meta.withColumnRenamed(\"genres\", \"genres_meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c522aaef-5a96-40af-ac53-54a994a07e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = [df_meta.tconst == df_TMDB.imdb_id]\n",
    "full = df_meta.join(df_TMDB, cond, 'outer')\n",
    "\n",
    "overviews = full.select(concat_ws(\" \", full.overview, \n",
    "                                       full.overview_meta, \n",
    "                                       full.Keywords).alias(\"FullOverview\"), \"imdb_id\", \"tconst\", \n",
    "                                                                             \"budget\", \"genres\", \"genres_meta\", \n",
    "                                                                             \"language\", \"popularity\", \"revenue\")\n",
    "\n",
    "overviews = overviews.withColumn('movie_id', coalesce('tconst', 'imdb_id')).drop(\"tconst\", \"imdb_id\")\n",
    "overviews = overviews.where(col(\"movie_id\").isin(set(ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391dd9e1-a877-4564-b16d-62f49fcbdbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = overviews.select(\"movie_id\", \"FullOverview\").join(training_data.select(\"tconst\", \"primaryTitleFormatted\"),\n",
    "                                                          overviews[\"movie_id\"] == training_data[\"tconst\"], \n",
    "                                                          how=\"outer\").drop(\"movie_id\")\n",
    "\n",
    "texts = texts.select(\"tconst\", concat_ws(\": \", \n",
    "                                         texts.primaryTitleFormatted,\n",
    "                                         texts.FullOverview).alias(\"FullText\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd76dd6-cddc-4b5d-a027-2f0a504d046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_TMDB = overviews.drop(\"FullOverview\", \"imdb_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6131e00c-f637-4868-a0d6-ea379b6d5753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return unicodedata.normalize('NFKD', text.lower()).encode('ascii', errors='ignore').decode('utf-8').replace(\"\\W\", \"\").replace(\"'\", \"\").replace('\"', \"\")\n",
    "        \n",
    "udf_clean_text = udf(lambda x: clean_text(x), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bab7a8-8eae-4562-9fcc-c3d11adf5108",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = texts.withColumn(\"FullText\", udf_clean_text(col(\"FullText\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93263e02-bff0-4cfe-9857-4122a01941c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts.write.mode(\"overwrite\").option(\"header\", True).csv(\"texts_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cd7c22",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Box Office data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f1b374-02ab-4d0c-a6c4-84c53f219744",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_office_schema = StructType() \\\n",
    "      .add(\"Rank\",IntegerType(),True) \\\n",
    "      .add(\"Release Group\",StringType(),True) \\\n",
    "      .add(\"Worldwide\",StringType(),True) \\\n",
    "      .add(\"Domestic\",StringType(),True) \\\n",
    "      .add(\"Col_to_Drop1\",StringType(),True) \\\n",
    "      .add(\"Foreign\",StringType(),True) \\\n",
    "      .add(\"Col_to_Drop2\",StringType(),True)\n",
    "\n",
    "n_skip_rows = 1\n",
    "box_office_rdd = spark.sparkContext \\\n",
    "    .textFile(\"box_office_mojo/*.csv\") \\\n",
    "    .zipWithIndex() \\\n",
    "    .filter(lambda row: row[1] >= n_skip_rows) \\\n",
    "    .map(lambda row: row[0])\n",
    "\n",
    "df_box_office_mojo = spark.read.csv(box_office_rdd, schema=box_office_schema, header=False)\n",
    "\n",
    "# process the 'release group' (read movie title) in the same way as the formatted title\n",
    "df_box_office_mojo = df_box_office_mojo.withColumn(\"Release Group\", lit(udf_format_titles('Release Group')))\n",
    "\n",
    "# add the year of the box office file\n",
    "df_box_office_mojo = df_box_office_mojo.withColumn(\"mojo_year\", substring(input_file_name(), -8, 4).cast(IntegerType()))\n",
    "\n",
    "# drop unnecessary columns\n",
    "df_box_office_mojo = df_box_office_mojo.drop(*('Col_to_Drop1', 'Col_to_Drop2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501d2746",
   "metadata": {},
   "source": [
    "# Adding of exogenous columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052d6fbf-96d9-4bfc-b257-d093dd111ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_incl_exog = df_preprocessed.copy(deep=True)\n",
    "# df_incl_exog = df_incl_exog.rename({\"tconst\" : \"id\"}, axis = 1).set_index(\"id\")\n",
    "# df_incl_exog.info()\n",
    "# training_data = training_data.withColumnRenamed('tconst', 'id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bd06e1",
   "metadata": {},
   "source": [
    "## add oscar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f96bdf0-384e-4869-96a3-0c7314a973da",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.join(oscar_noms, ['tconst'], 'left').withColumnRenamed('count', 'oscar_noms')\n",
    "training_data = training_data.join(oscar_wins, ['tconst'], 'left').withColumnRenamed('count', 'oscar_wins')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a84eb1c",
   "metadata": {},
   "source": [
    "## add razzie data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c66a93-98ba-4426-aff8-2b825d95f0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.join(razzie_noms, ['tconst'], 'left').withColumnRenamed('count', 'razzie_noms')\n",
    "training_data = training_data.join(razzie_wins, ['tconst'], 'left').withColumnRenamed('count', 'razzie_wins')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c17320-e41c-40a1-a9b1-2f9ddb04c8ee",
   "metadata": {},
   "source": [
    "## add TMDB & Metacritic data\n",
    "\n",
    "THIS DOES NOT INCLUDE THE OVERVIEWS, THEY WILL BE ADDED LATER, AFTER BEING CONVERTED TO D2V!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493c67d8-15fd-4f5a-be96-e2db5e611954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using overviews2, since overviews causes memory issues\n",
    "cond = [training_data.tconst == meta_TMDB.movie_id]\n",
    "meta_TMDB = meta_TMDB.withColumnRenamed(\"genres\",\"genres_tmdb\")\n",
    "training_data = training_data.join(meta_TMDB, cond, \"leftouter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9105936a-7847-4381-82e2-109de7d7e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac09b811",
   "metadata": {},
   "source": [
    "## add mojo box office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf2928b-b2f7-42e4-b23b-24ac8608b7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missing_box_office_values(column):\n",
    "    return when(column != '-', column).otherwise(lit(None))\n",
    "\n",
    "cond_mojo_merge = [training_data.primaryTitleFormatted == df_box_office_mojo['Release Group'], training_data.Year == df_box_office_mojo[\"mojo_year\"]]\n",
    "\n",
    "training_data = training_data.join(df_box_office_mojo, cond_mojo_merge, 'left').drop(*('Release Group', \"mojo_year\"))\n",
    "training_data = training_data.withColumn(\"Worldwide\", remove_missing_box_office_values(col(\"Worldwide\")))\n",
    "training_data = training_data.withColumn(\"Domestic\", remove_missing_box_office_values(col(\"Domestic\")))\n",
    "training_data = training_data.withColumn(\"Foreign\", remove_missing_box_office_values(col(\"Foreign\")))\n",
    "training_data = training_data.withColumn('Worldwide', regexp_replace('Worldwide', '[$,]', '').cast('double'))\n",
    "training_data = training_data.withColumn('Domestic', regexp_replace('Domestic', '[$,]', '').cast('double'))\n",
    "training_data = training_data.withColumn('Foreign', regexp_replace('Foreign', '[$,]', '').cast('double'))\n",
    "# training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451a5bcc-f3cb-4d28-88b8-41a4d9363942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data = training_data.withColumn('Year', coalesce('startYear', 'endYear'))\n",
    "\n",
    "training_data = training_data.withColumn('Revenue', coalesce('Revenue', 'Worldwide'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dde5c38-45e5-4842-bf33-3eeb6cd85bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264c2789",
   "metadata": {},
   "source": [
    "## add remake column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a232b91c-fa98-4458-be27-404e008ebf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.join(\n",
    "    training_data.groupBy(\"primaryTitle\").agg((count(\"*\")>1).cast(\"int\").alias(\"hasRemake\")),\n",
    "    on=\"primaryTitle\",\n",
    "    how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec691609",
   "metadata": {},
   "source": [
    "## add title language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c2e168-adec-402e-bdaf-54ec9ffaf697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add the language of the original title, currently commented for training data usage and not wait 15 min every time\n",
    "# df_incl_exog = add_language_of_original_title(df_incl_exog)\n",
    "\n",
    "# df_added_lang = pd.read_csv('additional_data/df_added_lang.csv', index_col=0)\n",
    "# df_added_lang = df_added_lang.rename({\"tconst\" : \"id\"}, axis = 1).set_index(\"id\")\n",
    "# df_incl_exog = df_incl_exog.join(df_added_lang['title_language'], how='left')\n",
    "\n",
    "added_lang_schema = StructType() \\\n",
    "      .add(\"_c0\",IntegerType(),True) \\\n",
    "      .add(\"tconst\",StringType(),True) \\\n",
    "      .add(\"primaryTitle\",StringType(),True) \\\n",
    "      .add(\"originalTitle\",StringType(),True) \\\n",
    "      .add(\"startYear\",IntegerType(),True) \\\n",
    "      .add(\"endYear\",IntegerType(),True) \\\n",
    "      .add(\"runtimeMinutes\",IntegerType(),True) \\\n",
    "      .add(\"numVotes\",FloatType(),True) \\\n",
    "      .add(\"label\",BooleanType(),True) \\\n",
    "      .add(\"title_language\",StringType(),True) \\\n",
    "      .add(\"isEN\",BooleanType(),True) \n",
    "\n",
    "n_skip_rows = 1\n",
    "added_lang_rdd = spark.sparkContext \\\n",
    "    .textFile('additional_data/df_added_lang.csv') \\\n",
    "    .zipWithIndex() \\\n",
    "    .filter(lambda row: row[1] >= n_skip_rows) \\\n",
    "    .map(lambda row: row[0])\n",
    "\n",
    "df_added_lang = spark.read.csv(added_lang_rdd, schema=added_lang_schema, header=False)\n",
    "\n",
    "training_data = training_data.join(df_added_lang.select(['tconst', 'title_language']), on='tconst', how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa17831",
   "metadata": {},
   "source": [
    "## add whether title is English or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3bab01-c3f6-4188-b4e1-6924a70a7a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indicate_whether_language_is_english(column):\n",
    "    return when(column == 'en', True).otherwise(lit(False))\n",
    "\n",
    "training_data = training_data.withColumn(\"isEN\", indicate_whether_language_is_english(col(\"title_language\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bc1096-ae04-40a7-8367-4a1fe9b279a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a68ab6",
   "metadata": {},
   "source": [
    "## add movie genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deae718-3950-472c-9973-29fbe3dedf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_genres = spark.read.csv(\"additional_data/movie_genres.csv\", header=True)\n",
    "movie_genres = movie_genres.filter(movie_genres.genres != '(no genres listed)')\n",
    "movie_genres = movie_genres.filter(movie_genres.title.endswith(')'))\n",
    "movie_genres = movie_genres.withColumn('year', substring(col('title'), -5, 4))\n",
    "movie_genres = movie_genres.filter(movie_genres.year != '')\n",
    "movie_genres = movie_genres.withColumn('year', col('year').cast(IntegerType()))\n",
    "movie_genres = movie_genres.withColumn('title', regexp_replace(col('title'), r' \\(.*?\\)', ''))\n",
    "# movie_genres = movie_genres.withColumn('genres', split(col('genres'), '\\|'))\n",
    "movie_genres = movie_genres.withColumn(\"titleFormatted\", lit(udf_format_titles('title')))\n",
    "movie_genres = movie_genres.dropDuplicates(['titleFormatted', 'year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e358db8-f769-46dd-828c-4cc8654e3e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_genres = ['Action', 'Adventure', 'Animation', 'Biography', 'Children', 'Comedy',\n",
    "       'Crime', 'Documentary', 'Drama', 'Family', 'Fantasy', 'Film-Noir', 'Horror', 'History',\n",
    "       'IMAX', 'Music', 'Musical', 'Mystery', 'News', 'Reality-TV','Romance', 'Sci-Fi', 'Science Fiction', 'Short', 'Sport', 'Thriller', 'TV Movie', 'War',\n",
    "       'Western']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94998d46-0024-4fd3-a4d9-df32c86a2b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cond = [training_data.Year == movie_genres.year, training_data.primaryTitleFormatted == movie_genres.titleFormatted]\n",
    "training_data = training_data.join(movie_genres.select(['year', 'titleFormatted', 'genres']), cond, how='left')\n",
    "# training_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd1d27a-0de6-4aa8-b89e-a6c911d2a368",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.withColumn('genres_combined', concat(col('genres'),col('genres_meta'),col('genres_tmdb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69a30ad-73ff-4f1a-b0cb-9e7aea0e0197",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in list_of_genres:\n",
    "    training_data = training_data.withColumn(c, col(\"genres_combined\").contains(c).cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17faa280-e2a5-4c21-8f0d-6c8c09be0077",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.drop(*('genres_combined', 'genres_meta', 'genres_tmdb', 'year', 'titleFormatted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30ddcb7",
   "metadata": {},
   "source": [
    "# Impute missing values\n",
    "### For columns that make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65cf5fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Expects an pyspark dataframe and either \"median\"/\"knn\"/\"mice\"\n",
    "\n",
    "def impute_missing(data, columns = ['_c0', 'tconst', 'numVotes', 'runtimeMinutes', 'budget', 'popularity', 'revenue'], strategy = 'median'):\n",
    "    data = data[columns]\n",
    "\n",
    "    \n",
    "    \n",
    "    #Get all columns types in lists\n",
    "    num_cols = [f.name for f in data.schema.fields if isinstance(f.dataType, FloatType)]\n",
    "    num_cols += [f.name for f in data.schema.fields if isinstance(f.dataType, IntegerType)]\n",
    "    num_cols += [f.name for f in data.schema.fields if isinstance(f.dataType, LongType)]\n",
    "    str_cols = [f.name for f in data.schema.fields if isinstance(f.dataType, StringType)]\n",
    "    bool_cols = [f.name for f in data.schema.fields if isinstance(f.dataType, BooleanType)]\n",
    "    cat_cols = str_cols + bool_cols\n",
    "    #num_imputed = [var + \"_imputed\" for var in num_cols]\n",
    "    \n",
    "    #fill categorical columns\n",
    "    data_df = data.toPandas().copy(deep=True)\n",
    "    data_df[cat_cols]=data_df[cat_cols].fillna(data_df.mode().iloc[0])\n",
    "    \n",
    "#purely in pyspark\n",
    "#     if strategy == 'median':\n",
    "#         imputer = Imputer(inputCols = num_cols, outputCols = num_imputed).setStrategy(\"median\")\n",
    "#         filled_data = imputer.fit(data).transform(data)\n",
    "#         return filled_data\n",
    "\n",
    "    #fill numerical columns with median\n",
    "    if strategy == 'median':\n",
    "        med_imputed = data[num_cols].toPandas().copy(deep=True)\n",
    "        med_imputer = SimpleImputer(strategy='median')\n",
    "        med_imputed.iloc[:, :] = med_imputer.fit_transform(med_imputed)\n",
    "        merged_df = pd.merge(data_df, med_imputed, how='right', on = '_c0', suffixes=(\"_l\", \"\")) #mice_imputed\n",
    "        final_df = merged_df[merged_df.columns[~merged_df.columns.str.endswith('_l')]]\n",
    "        final_df[num_cols] = final_df[num_cols].astype(int)\n",
    "        return spark.createDataFrame(final_df)\n",
    "\n",
    "    #fill numerical columns with KNN\n",
    "    elif strategy == \"KNN\":\n",
    "        knn_imputed = data[num_cols].toPandas().copy(deep=True)\n",
    "        knn_imputer = KNN()\n",
    "        knn_imputed.iloc[:, :] = knn_imputer.fit_transform(knn_imputed)\n",
    "        merged_df = pd.merge(data_df, knn_imputed, how='right', on = '_c0', suffixes=(\"_l\", \"\")) #mice_imputed\n",
    "        final_df = merged_df[merged_df.columns[~merged_df.columns.str.endswith('_l')]]\n",
    "        final_df[num_cols] = final_df[num_cols].astype(int)\n",
    "        final_df[str_cols] = final_df[str_cols].astype(str)\n",
    "        final_df[bool_cols] = final_df[bool_cols].astype(bool)\n",
    "        return spark.createDataFrame(final_df)\n",
    "    \n",
    "    #fill numerical columns with MICE\n",
    "    elif strategy == \"MICE\":\n",
    "        mice_imputed = data[num_cols].toPandas().copy(deep=True)\n",
    "        mice_imputer = IterativeImputer()\n",
    "        mice_imputed.iloc[:, :] = mice_imputer.fit_transform(mice_imputed)\n",
    "        merged_df = pd.merge(data_df, mice_imputed, how='right', on = '_c0', suffixes=(\"_l\", \"\")) #mice_imputed\n",
    "        final_df = merged_df[merged_df.columns[~merged_df.columns.str.endswith('_l')]]\n",
    "        final_df[num_cols] = final_df[num_cols].astype(int)\n",
    "        final_df[str_cols] = final_df[str_cols].astype(str)\n",
    "        final_df[bool_cols] = final_df[bool_cols].astype(bool)\n",
    "        return spark.createDataFrame(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c59cd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data = impute_missing(training_data, strategy = \"MICE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b402289-fe6f-42d6-82cc-21c9b0801117",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.join(imputed_data, on='tconst', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087c639b-e295-410e-a203-c48cda408f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.write.mode(\"overwrite\").option(\"header\",True).csv(\"validation_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eedb0b6",
   "metadata": {},
   "source": [
    "# Preparing data for classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1874804",
   "metadata": {},
   "source": [
    "Convert non-numeric columns to numeric:\n",
    "\n",
    "- Categorical column 'title language' is factorized\n",
    "- Categorical column 'language' is one-hot encoded\n",
    "- Missing values from columns 'startYear' and 'endYear' are cross filled\n",
    "- Missing values in columns 'oscar_noms' and 'oscar_wins' are replaced with zeroes\n",
    "- String columns ('primaryTitleFormatted', 'originalTitle', 'FullText') are embedded via Doc2Vec into an n-by-128 array\n",
    "\n",
    "### Genres columns are discarded, as at the current time the genres merging and encoding doesn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e42920a-d0b7-4e11-b090-c5e0d4da7fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_model_prep(data_filename: str, texts_filename: str, imputed_filename: str):\n",
    "    \n",
    "    # read data\n",
    "    df = pd.read_csv(f'{data_filename}.csv', index_col = 0)\n",
    "    df = df[df.index != \"tconst\"]\n",
    "\n",
    "    # read imputed data\n",
    "    imputed_df = pd.read_csv(f'{imputed_filename}.csv', index_col = 1)\n",
    "    \n",
    "    df[\"genres\"] = df[\"genres\"].str.split()\n",
    "\n",
    "    # This is extremely ugly, but I extremely do not care\n",
    "    df[\"genres_meta\"] = df[\"genres_meta\"].fillna(\"\").str.replace(\"[\", \"\").str.replace(\"]\", \"\").str.replace(\"'\", \"\").str.replace(\",\", \"\").str.split()\n",
    "    df['genres_meta'] = df['genres_meta'].apply(lambda d: d if isinstance(d, list) else [])\n",
    "    df['genres'] = df['genres'].apply(lambda d: d if isinstance(d, list) else [])\n",
    "\n",
    "    # Merge lists\n",
    "    df[\"genres_merged\"] = (df[\"genres\"] + df[\"genres_meta\"]).apply(set).apply(list)\n",
    "    mlb = MultiLabelBinarizer()\n",
    "\n",
    "    # One-hot encode\n",
    "    df = df.join(pd.DataFrame(\n",
    "                 mlb.fit_transform(df.pop('genres_merged')),\n",
    "                 index=df.index,\n",
    "                 columns=mlb.classes_), rsuffix = \"_genre\")\n",
    "    \n",
    "    print(\"Read files.\")\n",
    "    \n",
    "    df.drop(columns = imputed_df.columns.values, inplace = True)\n",
    "    df = df.join(imputed_df)\n",
    "    del imputed_df\n",
    "    \n",
    "    print(\"Deleted garbage\")\n",
    "    \n",
    "    df['title_language'] = pd.factorize(df['title_language'])[0]\n",
    "    \n",
    "    print(\"Factorized title language\")\n",
    "    \n",
    "    df['language'] = pd.factorize(df['language'])[0]\n",
    "    \n",
    "    lang_proc = df[\"language\"]\\\n",
    "                .replace(np.nan, \" \")\\\n",
    "                .apply(lambda x: re.sub(\"[^a-zA-Z]\", \" \", str(x)))\\\n",
    "                .str.split()\n",
    "\n",
    "    lang_proc = pd.DataFrame(mlb.fit_transform(lang_proc),\n",
    "                             columns=mlb.classes_,\n",
    "                             index=df.index)\n",
    "\n",
    "    \n",
    "    # dealing with (some) nan values\n",
    "    for index, row in df.iterrows():\n",
    "        # For missing startYear and endYear entries, insert the other, if it exists.\n",
    "        if math.isnan(row['startYear']):\n",
    "            if not math.isnan(row['endYear']):\n",
    "                df.at[index,'startYear']=df.at[index,'endYear']\n",
    "        if math.isnan(row['endYear']):\n",
    "            if not math.isnan(row['startYear']):\n",
    "                df.at[index,'endYear']=df.at[index,'startYear']\n",
    "\n",
    "        # For missing oscar_noms and oscar_wins, insert 0\n",
    "        if math.isnan(row['oscar_noms']):\n",
    "            df.at[index,'oscar_noms'] = 0\n",
    "        if math.isnan(row['oscar_wins']):\n",
    "            df.at[index,'oscar_wins'] = 0\n",
    "    \n",
    "    if \"d2v_model_trained.csv\" not in os.listdir():\n",
    "        d2v_train = pd.read_csv(f'data/texts_df_train.csv', index_col = 0)\n",
    "        d2v_valid = pd.read_csv(f'data/texts_df_valid.csv', index_col = 0)\n",
    "        d2v_test = pd.read_csv(f'data/texts_df_test.csv', index_col = 0)\n",
    "\n",
    "        df_d2v = pd.concat([d2v_train, d2v_valid, d2v_test], axis=0)\n",
    "\n",
    "        # This is gonna take a while\n",
    "        texts = d2v_embed(df_d2v[\"FullText\"])\n",
    "        texts.to_csv(f'd2v_model_trained.csv')\n",
    "    else:\n",
    "        # read texts data\n",
    "        texts = pd.read_csv(f'd2v_model_trained.csv', index_col = 0)\n",
    "    \n",
    "    print(\"D2V\")\n",
    "    \n",
    "    df.drop(columns=['genres', \n",
    "                     'genres_meta',\n",
    "                     'primaryTitle',\n",
    "                     'movie_id',\n",
    "                     'primaryTitleFormatted', \n",
    "                     'originalTitle', \n",
    "                     'language', \n",
    "                     'startYear',\n",
    "                     'endYear',\n",
    "                     '_c0'], inplace=True)\n",
    "    \n",
    "    # df = df.join(prim_title_df)\n",
    "    # df = df.join(orig_title_df)\n",
    "    df = df.join(text_df)\n",
    "    df = df.join(lang_proc)\n",
    "    \n",
    "    print(\"Finished\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082a2b15-3629-4366-ad26-e220e71cf00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_model_prep('data/training_df', 'data/texts_df_train', 'data/imputed_training_df')\n",
    "train_df.to_csv(\"completed_train_df1.csv\")\n",
    "\n",
    "eval_df = df_model_prep('data/validation_df', 'data/texts_df_valid', 'data/imputed_valid_df')\n",
    "eval_df.to_csv(\"completed_eval_df1.csv\")\n",
    "\n",
    "test_df = df_model_prep('data/test_df', 'data/texts_df_test', 'data/imputed_test_df')\n",
    "test_df.to_csv(\"completed_test_df1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19a7ad9",
   "metadata": {},
   "source": [
    "# Evaluating classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140b9241",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgbm = lgb.LGBMClassifier(objective='binary',\n",
    "                                learning_rate=0.001,\n",
    "                                num_iterations=5000,\n",
    "                                feature_fraction=0.8,\n",
    "                                verbosity=1,\n",
    "                                random_state=17)\n",
    "model_lgbm.fit(train_df.loc[:, train_df.columns != 'label'],\n",
    "              train_df['label'],\n",
    "              eval_metric='logloss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f11fafd",
   "metadata": {},
   "source": [
    "# Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88c5a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = df_model_prep('validation_df', 'texts_df_valid', 'imputed_valid_df')\n",
    "valid_df.drop(columns = ['label'], inplace = True)\n",
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49bf212",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df_model_prep('test_df', 'texts_df_test', 'imputed_test_df')\n",
    "test_df.drop(columns = ['label'], inplace = True)\n",
    "test_df_prepped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12194476",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds_lgbm = model_lgbm.predict(valid_df)\n",
    "with open('val_preds_lgbm.txt', 'w+') as f:\n",
    "    for val in val_preds_lgbm:\n",
    "        f.write(f\"{str(val)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461be441",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_lgbm = model_lgbm.predict(test_df)\n",
    "with open('test_preds_lgbm.txt', 'w+') as f:\n",
    "    for val in test_preds_lgbm:\n",
    "        f.write(f\"{str(val)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
