{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1002b2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c403250e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c735335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"completed_train_df1.csv\").set_index(\"tconst\")\n",
    "\n",
    "df[\"Domestic\"] = df[\"Domestic\"].fillna(df[\"Domestic\"].quantile(0.25))\n",
    "df[\"Foreign\"] = df[\"Foreign\"].fillna(df[\"Foreign\"].quantile(0.25))\n",
    "\n",
    "df[\"Worldwide\"] = df[\"Worldwide\"].fillna(df[\"Domestic\"] + df[\"Foreign\"])\n",
    "\n",
    "df = df.fillna(df.fillna(0).median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23de7fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataloader(Dataset):\n",
    "\n",
    "    def __init__(self, df, kind=\"train\"):\n",
    "        \n",
    "        x = df.drop([\"label\"], axis=1).values.astype(float)\n",
    "        y = df[\"label\"].values\n",
    "        \n",
    "        if kind == \"train\":\n",
    "            x = x[:6000]\n",
    "            y = y[:6000]\n",
    "        elif kind == \"eval\":\n",
    "            x = x[6000:]\n",
    "            y = y[6000:]\n",
    "\n",
    "        self.x_train = torch.tensor(x, dtype=torch.float32)\n",
    "        self.y_train = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "  \n",
    "    def __getitem__(self,idx):\n",
    "        return self.x_train[idx], self.y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110ed561",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = dataloader(df, \"train\")\n",
    "data_eval = dataloader(df, \"eval\")\n",
    "\n",
    "train_loader = DataLoader(data_train, batch_size=256)\n",
    "val_loader = DataLoader(data_eval, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1510695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(trainloader, valloader, model, criterion, optimizer):\n",
    "    \n",
    "    # Train on GPU if available\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(500):\n",
    "        for batch, (movie, label) in enumerate(trainloader):\n",
    "            # Compute prediction and loss\n",
    "            movie, label = movie.to(device), label.to(device)            \n",
    "            pred = model(movie)\n",
    "            loss = criterion(pred, label)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "#             # Print progress\n",
    "#             if batch % 5 == 0:\n",
    "#                 loss, current = loss.item(), batch * len(movie)\n",
    "#                 print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}] epoch: {epoch + 1}\")\n",
    "\n",
    "        # Print performance after current number of epochs\n",
    "        # print(f\"Epoch: {epoch + 1}\")\n",
    "        current_acc, avg_loss = test_loop(valloader, model, criterion, epoch, kind=\"eval\")\n",
    "        if epoch % 100 == 0:\n",
    "            current_acc_t, avg_loss_t = test_loop(trainloader, model, criterion, epoch, kind=\"train\")\n",
    "            time.sleep(2.5)\n",
    "            \n",
    "    print(f\"\\n\\n Final accuracy (eval): {current_acc}\\nFinal accuracy (train): {current_acc_t}\\n\")\n",
    "\n",
    "def test_loop(dataloader, model, criterion, epoch, kind=\"eval\"):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for movie, label in dataloader:\n",
    "            movie, label = movie.to(device), label.to(device)\n",
    "            pred = model(movie)\n",
    "            test_loss += criterion(pred, label).item()\n",
    "            correct += (pred.argmax(1) == label).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Epoch: {epoch}, {kind} Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\", end=\"\\r\")\n",
    "    return (100 * correct), test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ada2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class imdbModel(nn.Module):\n",
    "    def __init__(self, num_layers, hidden_size, drop_out=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(171, hidden_size)\n",
    "        self.fc_mid = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_final = nn.Linear(hidden_size, 2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.num_layers = num_layers\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        for _ in range(self.num_layers):\n",
    "            if drop_out:\n",
    "                x = self.dropout(x)\n",
    "            x = self.fc_mid(x)\n",
    "            x = self.relu(x)\n",
    "            \n",
    "        x = self.fc_final(x)\n",
    "                   \n",
    "        return self.logsoftmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b860af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for num_layers in [10]:\n",
    "#     for hidden_size in [1000]:\n",
    "#         for drop_out in [False, True]:\n",
    "#             model = imdbModel(num_layers, hidden_size, drop_out=drop_out)\n",
    "\n",
    "#             optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#             criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#             train_loop(train_loader, val_loader, model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac401fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c122047",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(df, train_size=0.9, shuffle=True, stratify=df['label'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3116f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df['label'] = valid_df['label'].astype('int')\n",
    "\n",
    "for num_leaves in [10, 30, 50, 100][::-1]:\n",
    "    for n_estimators in [1, 10, 100, 200][::-1]:\n",
    "        for max_depth in [-1, 1, 5, 10]:\n",
    "            for boosting in [\"gbdt\", \"dart\", \"goss\"]:\n",
    "                print(f\"Current: {num_leaves} leaves, {n_estimators} estimators, {max_depth} max depth, {boosting} boosting\", end=\": \")\n",
    "                \n",
    "                model_lgbm = lgb.LGBMClassifier(objective='binary',\n",
    "                                                boosting=boosting,\n",
    "                                                learning_rate=0.001,\n",
    "                                                num_iterations=5000,\n",
    "                                                num_leaves=num_leaves,\n",
    "                                                n_estimators=n_estimators,\n",
    "                                                max_depth=max_depth,\n",
    "                                                # feature_fraction=0.8,\n",
    "                                                # verbosity=1,\n",
    "                                                random_state=17,\n",
    "                                                n_jobs=-1);\n",
    "\n",
    "                model_lgbm.fit(train_df.drop(\"label\", axis=1),\n",
    "                               train_df['label'],\n",
    "                               eval_metric='logloss')\n",
    "                \n",
    "                val_preds = model_lgbm.predict(valid_df.drop(\"label\", axis=1))\n",
    "                print(accuracy_score(y_true=valid_df['label'].astype('int'), y_pred=val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568d92dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal (from what I found so far) ~ 0.806\n",
    "model_lgbm = lgb.LGBMClassifier(objective='binary',\n",
    "                                boosting=boosting,\n",
    "                                learning_rate=0.001,\n",
    "                                num_iterations=5000,\n",
    "                                num_leaves=50,\n",
    "                                n_estimators=200,\n",
    "                                max_depth=10,\n",
    "                                # feature_fraction=0.8,\n",
    "                                # verbosity=1,\n",
    "                                random_state=17,\n",
    "                                n_jobs=-1);\n",
    "\n",
    "model_lgbm.fit(train_df.drop(\"label\", axis=1),\n",
    "               train_df['label'],\n",
    "               eval_metric='logloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca91ce48",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = model_lgbm.predict(valid_df.drop(\"label\", axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2799ace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true=valid_df['label'].astype('int'), y_pred=val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16546524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dbea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_df.drop(\"label\", axis=1), label=train_df[\"label\"])\n",
    "dvalid = xgb.DMatrix(valid_df.drop(\"label\", axis=1), label=valid_df[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6dd9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'booster': \"gbtree\", 'max_depth': 1, 'eta': 1, 'objective': 'binary:logistic'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769bc038",
   "metadata": {},
   "outputs": [],
   "source": [
    "evallist = [(dvalid, 'eval'), (dtrain, 'train')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79a916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = 10\n",
    "bst = xgb.train(param, dtrain, num_round, evallist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f75c84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.round(bst.predict(dvalid), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72dfc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true=valid_df['label'].astype('int'), y_pred=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e075a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation = pd.read_csv(\"completed_eval_df1.csv\").set_index(\"tconst\")\n",
    "\n",
    "df_validation[\"Domestic\"] = df_validation[\"Domestic\"].fillna(df_validation[\"Domestic\"].quantile(0.25))\n",
    "df_validation[\"Foreign\"] = df_validation[\"Foreign\"].fillna(df_validation[\"Foreign\"].quantile(0.25))\n",
    "\n",
    "df_validation[\"Worldwide\"] = df_validation[\"Worldwide\"].fillna(df_validation[\"Domestic\"] + df_validation[\"Foreign\"])\n",
    "\n",
    "df_validation = df_validation.fillna(df.fillna(0).median()).drop([\"label\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c0407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation[['Adult', 'Biography', 'Film-Noir', 'Foreign_genre', 'Musical', 'News', 'Sci-Fi', 'Sport']] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0483033",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"completed_test_df1.csv\").set_index(\"tconst\")\n",
    "\n",
    "df_test[\"Domestic\"] = df_test[\"Domestic\"].fillna(df_test[\"Domestic\"].quantile(0.25))\n",
    "df_test[\"Foreign\"] = df_test[\"Foreign\"].fillna(df_test[\"Foreign\"].quantile(0.25))\n",
    "\n",
    "df_test[\"Worldwide\"] = df_test[\"Worldwide\"].fillna(df_test[\"Domestic\"] + df_test[\"Foreign\"])\n",
    "\n",
    "df_test = df_test.fillna(df.fillna(0).median()).drop([\"label\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fd28f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[['Adult', 'Biography', 'Film-Noir', 'Musical', 'News', 'Sci-Fi', 'Sport']] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edb645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds_lgbm = model_lgbm.predict(df_validation)\n",
    "with open('../val_preds_lgbm.txt', 'w+') as f:\n",
    "    for val in val_preds_lgbm:\n",
    "        f.write(f\"{str(val)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdfa1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_lgbm = model_lgbm.predict(df_test)\n",
    "with open('../test_preds_lgbm.txt', 'w+') as f:\n",
    "    for val in val_preds_lgbm:\n",
    "        f.write(f\"{str(val)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d457e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds_lgbm.mean(), test_preds_lgbm.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e5773b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
