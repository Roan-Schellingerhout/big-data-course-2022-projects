{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "caef4d74-2bde-4ef7-85ab-78e29daaffc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (2.27.1)\n",
      "Collecting requests-html\n",
      "  Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (1.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests) (2.0.10)\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting w3lib\n",
      "  Downloading w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n",
      "Collecting parse\n",
      "  Downloading parse-1.19.0.tar.gz (30 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyppeteer>=0.0.14\n",
      "  Downloading pyppeteer-1.0.2-py3-none-any.whl (83 kB)\n",
      "     |████████████████████████████████| 83 kB 1.6 MB/s            \n",
      "\u001b[?25hCollecting pyquery\n",
      "  Downloading pyquery-1.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting fake-useragent\n",
      "  Downloading fake-useragent-0.1.11.tar.gz (13 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.19.2 in /opt/conda/lib/python3.9/site-packages (from pandas) (1.21.5)\n",
      "Collecting pyee<9.0.0,>=8.1.0\n",
      "  Downloading pyee-8.2.2-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in /opt/conda/lib/python3.9/site-packages (from pyppeteer>=0.0.14->requests-html) (4.10.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /opt/conda/lib/python3.9/site-packages (from pyppeteer>=0.0.14->requests-html) (4.62.3)\n",
      "Collecting websockets<11.0,>=10.0\n",
      "  Downloading websockets-10.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (105 kB)\n",
      "     |████████████████████████████████| 105 kB 9.4 MB/s            \n",
      "\u001b[?25hCollecting appdirs<2.0.0,>=1.4.3\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.9/site-packages (from bs4->requests-html) (4.10.0)\n",
      "Collecting cssselect>0.7.9\n",
      "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting lxml>=2.1\n",
      "  Downloading lxml-4.8.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.manylinux_2_24_aarch64.whl (6.6 MB)\n",
      "     |████████████████████████████████| 6.6 MB 6.9 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.7.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.9/site-packages (from beautifulsoup4->bs4->requests-html) (2.3.1)\n",
      "Building wheels for collected packages: bs4, fake-useragent, parse\n",
      "  Building wheel for bs4 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1272 sha256=dfff3fb5f4d64a4df651f9a014ce6cbebe2c93dd28c57a802a16a74d71282d2b\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/73/2b/cb/099980278a0c9a3e57ff1a89875ec07bfa0b6fcbebb9a8cad3\n",
      "  Building wheel for fake-useragent (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-py3-none-any.whl size=13502 sha256=547c692499e9e0072cc6e289b31cc4b7a7a8c4b0b50a5d3f93287ebf147b19cf\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/ae/e7/76/7dd44644d065268ab0e1b4fa2e802fa4bb0157717b7d6c6d92\n",
      "  Building wheel for parse (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for parse: filename=parse-1.19.0-py3-none-any.whl size=24591 sha256=cebc5a2c8aecae926b8bc368f592e3092c19a4bba0310d662b250ef424d03ce3\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/d6/9c/58/ee3ba36897e890f3ad81e9b730791a153fce20caa4a8a474df\n",
      "Successfully built bs4 fake-useragent parse\n",
      "Installing collected packages: websockets, pyee, lxml, cssselect, appdirs, w3lib, pyquery, pyppeteer, parse, fake-useragent, bs4, requests-html\n",
      "Successfully installed appdirs-1.4.4 bs4-0.0.1 cssselect-1.1.0 fake-useragent-0.1.11 lxml-4.8.0 parse-1.19.0 pyee-8.2.2 pyppeteer-1.0.2 pyquery-1.4.3 requests-html-0.10.0 w3lib-1.22.0 websockets-10.2\n"
     ]
    }
   ],
   "source": [
    "!pip install requests requests-html pandas\n",
    "\n",
    "# if python < python3.7\n",
    "# pip install dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "adb26d41-5f50-4141-b0e6-e6ce1f12d6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests_html import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc58b5f3-7472-4de7-8441-b75789c2f286",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ScrapeBoxOffice:\n",
    "    base_endpoint:str = \"https://www.boxofficemojo.com/year/world/\"\n",
    "    year:int = None\n",
    "    save_raw:bool = False\n",
    "    save:bool = False\n",
    "    output_dir: str = \".\"\n",
    "    table_selector: str = '.imdb-scroll-table'\n",
    "    table_data = []\n",
    "    table_header_names = []\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.year if isinstance(self.year, int) else 'world'\n",
    "    \n",
    "    def get_endpoint(self):\n",
    "        endpoint = self.base_endpoint\n",
    "        if isinstance(self.year, int):\n",
    "            endpoint = f\"{endpoint}{self.year}/\"\n",
    "        return endpoint\n",
    "    \n",
    "    def get_output_dir(self):\n",
    "        return pathlib.Path(self.output_dir)\n",
    "    \n",
    "    def extract_html_str(self, endpoint=None):\n",
    "        url = endpoint if endpoint is not None else self.get_endpoint()\n",
    "        r = requests.get(url, stream=True)\n",
    "        html_text = None\n",
    "        status = r.status_code\n",
    "        if r.status_code == 200:\n",
    "            html_text = r.text\n",
    "            if self.save_raw:\n",
    "                output_fname = f\"{self.name}.html\"\n",
    "                raw_output_dir = self.get_output_dir() / 'html'\n",
    "                raw_output_dir.mkdir(exist_ok=True, parents=True)\n",
    "                output_fname = raw_output_dir / output_fname\n",
    "                with open(f\"{output_fname}\", 'w') as f:\n",
    "                    f.write(html_text)\n",
    "            return html_text, status\n",
    "        return html_text, status\n",
    "    \n",
    "    def parse_html(self, html_str=''):\n",
    "        r_html = HTML(html=html_str)\n",
    "        r_table = r_html.find(self.table_selector)\n",
    "        if len(r_table) == 0:\n",
    "            return None\n",
    "        table_data = []\n",
    "        header_names = []\n",
    "        parsed_table = r_table[0]\n",
    "        rows = parsed_table.find(\"tr\")\n",
    "        header_row = rows[0]\n",
    "        header_cols = header_row.find('th')\n",
    "        header_names = [x.text for x in header_cols]\n",
    "        for row in rows[1:]:\n",
    "            cols = row.find(\"td\")\n",
    "            row_data = []\n",
    "            row_dict_data = {}\n",
    "            for i, col in enumerate(cols):\n",
    "                header_name = header_names[i]\n",
    "                row_data.append(col.text)\n",
    "            table_data.append(row_data)\n",
    "        self.table_data = table_data\n",
    "        self.table_header_names = header_names\n",
    "        return self.table_data, self.table_header_names\n",
    "    \n",
    "    def to_df(self, data=[], columns=[]):\n",
    "        return pd.DataFrame(data, columns=columns)\n",
    "    \n",
    "    def run(self, save=False):\n",
    "        save = self.save if save is False else save\n",
    "        endpoint = self.get_endpoint()\n",
    "        html_str, status = self.extract_html_str(endpoint=endpoint)\n",
    "        if status not in range(200, 299):\n",
    "            raise Exception(f\"Extraction failed, endpoint status {status} at {endpoint}\")\n",
    "        data, headers = self.parse_html(html_str if html_str is not None else '')\n",
    "        df = self.to_df(data=data, columns=headers)\n",
    "        self.df = df\n",
    "        if save:\n",
    "            filepath = self.get_output_dir() / f'{self.name}.csv'\n",
    "            df.to_csv(filepath, index=False)\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "487179da-2859-424c-96ff-3c1433dc30c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(1985, 2023):\n",
    "    scraper = ScrapeBoxOffice(year=year, save=True, save_raw=True, output_dir='box_office_mojo')\n",
    "    df = scraper.run()\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f9c804-f8ad-42e5-b8a7-e0046ae2acd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
